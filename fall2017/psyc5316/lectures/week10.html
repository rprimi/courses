<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>Week 10 lecture notes - PSYC 5316</title>
<!-- 2017-10-30 Mon 06:23 -->
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="generator" content="Org-mode" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center; }
  .todo   { font-family: monospace; color: red; }
  .done   { color: green; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.right  { text-align: center;  }
  th.left   { text-align: center;   }
  th.center { text-align: center; }
  td.right  { text-align: right;  }
  td.left   { text-align: left;   }
  td.center { text-align: center; }
  dt { font-weight: bold; }
  .footpara:nth-child(2) { display: inline; }
  .footpara { display: block; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/javascript" src="http://orgmode.org/mathjax/MathJax.js"></script>
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
    MathJax.Hub.Config({
        // Only one of the two following lines, depending on user settings
        // First allows browser-native MathML display, second forces HTML/CSS
        //  config: ["MMLorHTML.js"], jax: ["input/TeX"],
            jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        showProcessingMessages: true,
        displayAlign: "center",
        displayIndent: "2em",

        "HTML-CSS": {
             scale: 100,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<h1 class="title">Week 10 lecture notes - PSYC 5316</h1>
<p>
Last week, we introduced Bayesian modeling through sampling the posterior.  This week, we will finish up our discussion of the modeling process by talking about using the model for <b>prediction</b>.
</p>

<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1">Evaluating Bayesian models</h2>
<div class="outline-text-2" id="text-1">
<p>
Last week, our computations gave us information about the plausible values of \(p\) for our binomial model.  For example, we computed an 80% HPDI for \(p\) to be [0.47,0.83], which means that \(p\) lies between 0.47 and 0.83 with probability 0.80.  In fact, we estimate the entire posterior distribution (i.e., the probability values for <b>all</b> \(p\) between 0 and 1).
</p>

<p>
We can now take this information and "go the other way".  That is, we can use the estimated parameter values \(p\) and estimate how likely a given data observation would be.  That is, our model is <i>generative</i> in the sense that we can generate predictions from the model.
</p>

<p>
To illustrate, let's assume that \(p=0.7\).  The following R code will perform 1000 simulations of our experiment (as a reminder, remember that we started with the game of tossing a globe 9 times and recording W or L..the <code>rbinom</code> will count the number of "successes" as water landings). Then, we'll plot a very simple type of histogram to show the relative frequency of each possible number of outcomes (0-9).
</p>

<pre class="example">
predictions = rbinom(1000, size=9, prob=0.7)
plot(table(predictions), xlim=c(0,9))
</pre>


<div class="figure">
<p><img src="figures/week9/predict1.jpeg" alt="predict1.jpeg" />
</p>
</div>

<p>
Note that with \(p=0.7\), we still get quite a large range of possible outcomes.  However, the outcomes \(x=6\) and \(x=7\) are still the most frequent (as we would expect, since 70% of 9 is 6.3).
</p>

<p>
As an exercise, you should play around with different values of \(p\), ranging from small values (e.g., \(p=0.1\)) to larger values (e.g., \(p=0.9\)).  What changes about the distribution of predictions?  
</p>

<p>
The R code below will show how the distribution of predictions changes for \(p\) from 0.1 to 0.9:
</p>

<pre class="example">
par(mfrow=c(1,5))
Ps = c(0.1,0.3,0.5,0.7,0.9)
for (i in 1:5){
  predictions = rbinom(1000, size=9, prob=Ps[i])
  plot(table(predictions), xlim=c(0,9), ylab="", main=paste("p=",Ps[i]))
}
par(mfrow=c(1,1))
</pre>


<div class="figure">
<p><img src="figures/week9/predictive.png" alt="predictive.png" />
</p>
</div>

<p>
However, our estimate for \(p\) is a <i>distribution</i> of values..not a single value.  Thus, we need to incorporate our uncertainty for \(p\) in our prediction as well.  The way to do this is through the <b>posterior predictive</b> distribution. 
</p>

<p>
Basically, the idea is as follows.  From the figure above, we can see that as \(p\) increases from 0 to 1, the peak of the predictive distribution shifts from the low end \(x=1\) to the high end \(x=9\).  However, from our posterior distribution of \(p\), we know that the probability that \(p\) lies on either of these ends is very small.  Thus, we need to incorporate this knowledge into our predictive distribitution.  
</p>

<p>
Essentially, we need to form a <i>weighted average</i> of predictions.  Predicted observations on the low end (\(x=0,1\)) come from small values of \(p\), which are not very common in our posterior distribution.  Similarly, predicted observations on the high end (\(x=8,9\)) come from large values of \(p\), which are also not very common in the posterior distribution.  When we weight the likelihood of the various observations by the relative posterior probabilities for \(p\), we get a result that we call the <b>posterior predictive distribution</b>.  The math can be complicated, but the R code is simple:
</p>

<pre class="example">
predictions = rbinom(1000, size=9, prob=samples)
plot(table(predictions), xlim=c(0,9))
</pre>


<div class="figure">
<p><img src="figures/week9/postPredictive.png" alt="postPredictive.png" />
</p>
</div>

<p>
Notice how the only bit of code that we changed is the <code>prob</code> value.  In the above samples, we used a fixed value of \(p\).  In the posterior predictive distribution, we set \(p\) to be a random variable that is shaped like our posterior distribution for \(p\).  Since we approximated this distribution with <code>samples</code> earlier, we can simply set <code>prob=samples</code> to accomplish this.
</p>

<p>
Notice how the posterior predictive distribution peaks around 6, which matches with our observed data (remember, we saw \(x=6\) water landings in our example).  Thus, we can conclude that the model is adequate in the sense that it predicts what we've already seen.  This predictive adequacy is a fundamental part of Bayesian modeling, and it is called a <b>posterior predictive check</b>.
</p>
</div>
</div>

<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2">Next step: MCMC sampling</h2>
<div class="outline-text-2" id="text-2">
<p>
So far, we have talked about how to construct a Bayesian model. We can summarize this into the following steps:
</p>

<ol class="org-ol">
<li>We begin with a <i>data story</i>; that is, we hypothesize a process by which our data might have arisen.  This results in a <i>likelihood</i> function.  
</li>
<li>We then decide on a prior, which represents our prior knowledge. 
</li>
<li>Then, we use Bayes theorem to update our model by <i>feeding</i> it the data. Technically, this amounts to computing a <i>posterior</i> by multiplying the prior and the likelihood.  Computationally, we use <i>sampling</i> to accomplish this.  
</li>
<li>Once we estimate our posterior distribution, we check our model by generating a <i>posterior predictive distribution</i>, which incorporates both data uncertainty as well as parameter uncertainty into a single distribution.  We compare this posterior predictive distribution to our observed data; if the observed data match what we would <i>expect</i> from the model (based on the posterior predictive), we can conclude that our model does an adequate job of describing our data.
</li>
</ol>

<p>
We will finish our intro to Bayesian modeling by discussing a more modern method of posterior sampling known as <b>Markov chain Monte Carlo</b> sampling.
</p>
</div>
</div>

<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3">The Metropolis algorithm</h2>
<div class="outline-text-2" id="text-3">
<p>
MCMC methods work by taking random samples from the posterior.  The MCMC method we will discuss is called the <b>Metropolis algorithm</b>.  It works by taking posterior samples in such a way that more time is spent wherever the posterior distribution is more dense.  In the long run (i.e., after many steps), the distribution of samples will then form a very good approximation to the actual posterior distribution.
</p>

<p>
To illustrate how the algorithm works, I take the approach used by Richard McElreath and describe a fictional story involving a king who wishes to travel around the islands in his kingdom.
</p>

<p>
King Markov ruled over an archipelago consisting of 10 islands, each arranged in a circle.  His main obligation was that the time spend visiting the people of each island should be proportional to the population of the island.  That is, if island 1 had twice as many people as island 2, then he should spend twice as many days on Island 1 as compared to Island 2.
</p>

<p>
An easy way to accomplish this would be to simply have a list of each Island's population.  However, the king had no use for such trivialities, and thus required his cabinet to develop a daily travel plan that would satisfy his obligation without having to remember or write down any island's population.
</p>

<p>
Senator Metropolis (an esteemed member of the cabinet who is also a very capable applied mathematician) developed the following algorithm:
</p>

<ol class="org-ol">
<li>Whereever the King is, each week he needs to decide between staying put for another day, or moving to one of the two adjacent islands.  To decide his next move, he flips a coin.
</li>

<li>If the coin turns up heads, the King <i>considers</i> moving to the adjacent island <i>clockwise</i>.  If it turns up tails, he considers moving <i>counterclockwise</i>.  This is called the <i>proposal</i> island.
</li>

<li>To decide whether he moves to the proposal island, he uses a rather interesting random sampling procedure.  He collects a number of white stones to represent the population of the <i>current</i> island, and a number of black stones to represent the population of the <i>proposal</i> island.  Then, he does the following:
<ul class="org-ul">
<li>if the number of black stones exceeds the number of white stones (that is, the relative population of the proposal island is larger than the current island), he <i>always</i> moves to the proposal island.
</li>
<li>if the number of black stones is less than the number of white stones, he, he removes a number of white stones equal to the number of black stones.  For example, if he had 4 black stones and 6 white stones, we would then remove 4 white stones to end with 2 white stones.  In other words, the ratio of black/white stones would be 4:2.
</li>
<li>finally, he reaches into the bag and pulls a stone.  If it is black, he moves to the proposal island.  If it is white, he stays. 
</li>
</ul>
</li>
</ol>

<p>
Believe it or not, this algorithm works!  To get the hang of it, we will try it in class for a few cycles. Then, we will use R to see how the algorithm works in the long run.
</p>

<p>
The following series of R code cunks will demonstrate the long-run behavior.
</p>

<p>
First, we define the population of each island. This is entirely arbitrary, and I invite you to play around with the values in <code>population</code>.
</p>

<pre class="example">
islandNumber = 1:10
populations = c(2,3,3,5,6,10,10,1,2,3)
plot(islandNumber, populations, type="h")
</pre>


<div class="figure">
<p><img src="figures/week10/islandPop.jpeg" alt="islandPop.jpeg" />
</p>
</div>

<p>
Now the algorithm is instantiated below.  Please try to read the code in light of the algorithm as presented above.  The key is that there is a random proposal island selected, and then we have to decide whether to "stay" or "go", which is done by a "coin flip" that is biased to come up heads according to the ratio of population densities (the so-called "acceptance ratio").
</p>

<p>
(also note that there is an extra step for THIS situation whereby we need to make the island chain circular..this won't be relevant to any other of the situations we talk about..it is simply to make this example work!)
</p>

<pre class="example">
# begin recording locations
N=10000
locations = numeric(N)
locations[1] = sample(1:10,1) # random starting place

# loop that defines Metropolis algorithm
for (i in 2:N){
  # take random walk either left or right
  proposal = locations[i-1] + sample(c(-1,1), 1)
  
  # make chain of islands circular
  if (proposal==11){proposal=1}
  if (proposal==0){proposal=10}
  
  # compute acceptance ratio (ratio of pop densities)
  current = populations[locations[i-1]]
  proposed = populations[proposal]
  acceptRatio = min(1, proposed/current)
  
  # move or stay according to acceptance ratio
  makeStep = rbinom(1, size=1, prob=acceptRatio)
  if (makeStep==0){
    locations[i]=locations[i-1]
  }
  else{
    locations[i]=proposal
  }
}
</pre>

<p>
The next two plots will how us that everything worked.  First, we plot the first 100 days worth of samples:
</p>

<pre class="example">
plot(locations[1:100])
</pre>


<div class="figure">
<p><img src="figures/week10/samples.jpeg" alt="samples.jpeg" />
</p>
</div>

<p>
Next, we plot the proportion of time spent on each island.  Notice how it closely resembles the original population density plot above.  Magic!
</p>

<pre class="example">
plot(table(locations))
</pre>


<div class="figure">
<p><img src="figures/week10/sampledPop.jpeg" alt="sampledPop.jpeg" />
</p>
</div>
</div>
</div>

<div id="outline-container-sec-4" class="outline-2">
<h2 id="sec-4">Using the Metropolis algorithm: the globe-tossing example</h2>
<div class="outline-text-2" id="text-4">
<p>
Now we are in a position to apply this new MCMC method to estimating the posterior distribution for \(p\) in our globe-tossing example.  Recall that \(p\) represents the proportion of water on the globe.  Also recall that we observed 6 "W" outcomes out of 9 globe tosses.
</p>

<p>
To construct the Bayesian model, we need the following two things:
</p>

<ol class="org-ol">
<li>the likelihood (the data story) &#x2013; we suppose that our data is generated by a binomial likelihood.  In symbols, we write:
</li>
</ol>

<p>
\[
x \sim \text{Binomial}(p)
\]
</p>

<ol class="org-ol">
<li>the prior for \(p\) &#x2013; as before, we will use a uniform prior to start.  In symbols, we write:
</li>
</ol>

<p>
\[
p\sim \text{Uniform}(0,1)
\]
</p>

<p>
By Bayes' Theorem we know that the posterior is found by multiplying the prior and the posterior, then dividing by a normalizing constant (so that the area under the distribution is 1).  To do this by exact mathematical methods is not difficult, but beyond the scope of this course.  Instead, we will use the Metropolis algorithm to sample from the posterior, then use density of the resulting "chain" of samples as our stand-in for the posterior.
</p>

<p>
As before, we need R to accomplish this.
</p>

<p>
First, we define the likelihood and prior.  This should seem similar to the way we defined functions when dealing with maximum likelihood estimation:
</p>

<pre class="example">
likelihood = function(data, par){
  return(dbinom(x=data, prob=par, size=9))
}

prior = function(data, par){
  return(dunif(x=par, min=0, max=1))
}
</pre>


<p>
Finally, we can define the posterior by simply multiplying these two functions:
</p>

<pre class="example">
posterior = function(data, par){
  return(likelihood(data, par)*prior(data, par))
}
</pre>

<p>
Now we can use the <code>posterior</code> function in the Metropolis algorithm.  We don't need to modify too much from the silly example we used above:
</p>

<pre class="example">
N=10000
samples = numeric(N)

# random starting place
samples[1] = runif(1, min=0, max=1)

for (i in 2:N){
  proposal = runif(1, min=0, max=1)

  # acceptance ratio
  current = posterior(data=6, par=samples[i-1])
  proposed = posterior(data=6, par=proposal)
  acceptRatio = min(1, proposed/current)
  
  # move or stay
  makeStep = rbinom(1, size=1, prob=acceptRatio)
  if (makeStep==0){
    samples[i]=samples[i-1]
  }
  else {
    samples[i]=proposal
  }
}
</pre>

<p>
As before, this results in a "chain" of posterior samples:
</p>

<pre class="example">
plot(samples, type="l")
</pre>


<div class="figure">
<p><img src="figures/week10/chain.jpeg" alt="chain.jpeg" />
</p>
</div>

<pre class="example">
plot(density(samples), lwd=2)
</pre>


<div class="figure">
<p><img src="figures/week10/posterior.jpeg" alt="posterior.jpeg" />
</p>
</div>

<p>
We can now do any computation we wish with the <code>samples</code> object in R, including computing HPDIs, posterior modes, etc.  Just refer to last week's lecture notes.
</p>
</div>

<div id="outline-container-sec-4-1" class="outline-3">
<h3 id="sec-4-1">Changing things up a bit</h3>
<div class="outline-text-3" id="text-4-1">
<p>
The first thing to play around with is the choice of prior.  What if, instead of a uniform prior, we wanted to use the "half-prior" that we used before, where all prior probability mass was on the interval (0.5,1).  All we need to change in the R code above is our definition of <code>prior</code>, which we can do as follows:
</p>

<pre class="example">
prior = function(data, par){
  return(dunif(x=par, min=0.5, max=1))
}
</pre>

<p>
After re-running the above code with the new choice of prior, we get the following posterior density:
</p>


<div class="figure">
<p><img src="figures/week10/posterior-half.jpeg" alt="posterior-half.jpeg" />
</p>
</div>

<p>
As before, since we have prior probability 0 for any parameter estimate below 0.5, we consequently have posterior probability 0 for such estimates.  Hence, all posterior mass is on the interval (0.5, 1).
</p>

<p>
The other thing I would like to explore is how we can modify the proposal function.  If you take a close look at the code for the Metropolis algorithm above, you'll notice that our proposals are generated via <code>runif</code>.  That is, we are randomly picking points in the interval (0,1) with equal probability.  For example, if our current sampled point was \(p=0.1\), we could very easily have a proposal point as \(p=0.9\).  So, the subsequent samples are not very close to each other.  Compare this to the example above, where the King only moved to an <i>adjacent</i> island.  
</p>

<p>
What if, instead, we chose points that were very close to the previous sample point?  We can do this by basing our proposal function on a normal distribution with very small standard deviation.
</p>

<p>
We can explore that by changing <code>proposal</code> to the following function:
</p>

<pre class="example">
x = rnorm(1, mean=samples[i-1], sd=0.1)
if (x&gt;0 &amp; x&lt;1){
  proposal=x
}else if (x&gt;1){
  proposal=1
}else if (x&lt;0){
  proposal=0
}
</pre>

<p>
To run the Metropolis algorithm with this new proposal function, simply comment out the original line that defines <code>proposal</code> and replace with the code chunk immediately above.  You should notice that you get a similar output when plotting the chain and the density plot of the samples.
</p>

<p>
However, there is a choice to be made.  Notice in the <code>rnorm</code> function, I chose <code>sd=0.1</code>.  This is essentially a choice for the "width" of the random walk.  What if I made it a lot smaller..say <code>sd=0.01</code>?  Try it!
</p>

<p>
Notice that the resulting chain plot is much different:
</p>


<div class="figure">
<p><img src="figures/week10/mixing.jpeg" alt="mixing.jpeg" />
</p>
</div>

<p>
This is an example of a chain which has not "mixed" well.  The steps of the random walk are not <i>random enough</i>..this means that you have not explored the parameter space nearly well enough.  Of course, this happened because we chose the width of the steps in the proposal function to be too small. We can see the result of this bad mixing in the following density plot:
</p>


<div class="figure">
<p><img src="figures/week10/notMixed.jpeg" alt="notMixed.jpeg" />
</p>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: October 30, 2017</p>
<p class="date">Created: 2017-10-30 Mon 06:23</p>
<p class="creator"><a href="http://www.gnu.org/software/emacs/">Emacs</a> 25.2.1 (<a href="http://orgmode.org">Org</a> mode 8.2.10)</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
