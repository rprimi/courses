<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>Week 9 lecture notes - PSYC 5316</title>
<!-- 2017-10-23 Mon 09:47 -->
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="generator" content="Org-mode" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center; }
  .todo   { font-family: monospace; color: red; }
  .done   { color: green; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.right  { text-align: center;  }
  th.left   { text-align: center;   }
  th.center { text-align: center; }
  td.right  { text-align: right;  }
  td.left   { text-align: left;   }
  td.center { text-align: center; }
  dt { font-weight: bold; }
  .footpara:nth-child(2) { display: inline; }
  .footpara { display: block; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/javascript" src="http://orgmode.org/mathjax/MathJax.js"></script>
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
    MathJax.Hub.Config({
        // Only one of the two following lines, depending on user settings
        // First allows browser-native MathML display, second forces HTML/CSS
        //  config: ["MMLorHTML.js"], jax: ["input/TeX"],
            jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        showProcessingMessages: true,
        displayAlign: "center",
        displayIndent: "2em",

        "HTML-CSS": {
             scale: 100,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<h1 class="title">Week 9 lecture notes - PSYC 5316</h1>
<p>
Last week, we introduced Bayes theorem, and began talking about Bayesian inference, using terms like <i>prior</i>, <i>posterior</i>, and <i>likelihood</i>.  Recall that Bayes theorem gives us the fundamental equation of <i>Bayesian updating</i>..that is:
</p>

<p>
\[
\text{posterior} \propto \text{likelihood} \times \text{prior}
\]
</p>

<p>
This wee, we will begin learning how to move beyond the <b>concept</b> of Bayesian updating to using it in a modeling context. 
</p>

<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1">Building Bayesian models</h2>
<div class="outline-text-2" id="text-1">
<p>
Bayesian modeling uses the basic vocabulary of Bayes theorem that we developed last week.  Our goal is to quantify our <b>posterior beliefs</b> in a model.  We do this by <b>updating</b> our <b>prior beliefs</b> via Bayes theorem&#x2026;that is, posterior = prior x likelihood.
</p>

<p>
Let's revisit the "globe tossing" example from the first exam.
</p>

<p>
Suppose you have a globe that represents the Earth.  You would like to estimate how much of the surface is covered in water.  To measure this, you adopt the following strategy: toss the globe up in the air. When you catch it, you will record whether the surface under your right index finger is water or land.  Then you toss the globe up in the air again and repeat the procedure. The first nine samples generate the following sequence:
</p>

<p>
W L W W W L W L W
</p>

<p>
where W indicates water and L indicates land. So in this example you observe six W (water) observations and three L (land) observations. Call this sequence of observations the <b>data</b>.
</p>

<p>
To construct a model, we need to make assumptions.  Designing a simple Bayesian model uses a design loop with three steps.
</p>

<ol class="org-ol">
<li>Data story: Motivate the model by narrating how the data might arise.
</li>
<li>Update: Educate your model by feeding it the data.
</li>
<li>Evaluate: All statistical models require supervision, leading possibly to model revision.  We'll talk about this later.
</li>
</ol>
</div>

<div id="outline-container-sec-1-1" class="outline-3">
<h3 id="sec-1-1">The data story</h3>
<div class="outline-text-3" id="text-1-1">
<p>
The "data story" amounts to explaining how each piece of data is born. This usually means describing aspects of the underlying reality as well as the sampling process. The data story in this case is simply a restatement of the sampling process:
</p>
<ul class="org-ul">
<li>Assume the true proportion of water covering the globe is p.
</li>
<li>A single toss of the globe has a probability p of producing a water (W) observation. It has a probability 1 âˆ’ p of producing a land (L) observation.
</li>
<li>Each toss of the globe is independent of the others.
</li>
</ul>

<p>
This is usually translated into a probability statement, called a <b>likelihood</b>.  Based on the story, we can use a <b>binomial</b> likelihood to model the data generation.
</p>

<p>
\[
f(x,N,p) = {N\choose x}p^{x}(1-p)^{1-x}
\]
</p>
</div>
</div>

<div id="outline-container-sec-1-2" class="outline-3">
<h3 id="sec-1-2">Updating</h3>
<div class="outline-text-3" id="text-1-2">

<div class="figure">
<p><img src="figures/week9/globeTossing.jpg" alt="globeTossing.jpg" />
</p>
</div>

<p>
In this figure, we see how Bayesian updating works.  
</p>
<ul class="org-ul">
<li>start with a prior that assigns equal probability to all values of \(p\) between 0 and 1.
</li>
<li>after seeing the first "W", our posterior changes.  Now, \(p=0\) is impossible (probability = 0), and \(p>0.5\) is much more likely than \(p<0.5\).
</li>
<li>after seeing the next data "L", the posterior changes again.  Both \(p=0\) and \(p=1\) are impossible, with \(p=0.5\) most likely (which makes sense, since out of TWO tosses, we've seen one W and one L).
</li>
<li>after seeing the next data "W", the posterior again shifts toward \(p=1\), but note that \(p=1\) is still impossible.
</li>
<li>each observation of "W" shifts the peak toward the right \(p=1\), while each observation of "L" shifts the peak toward the left.
</li>
</ul>

<p>
From this, we can think of a Bayesian model as a machine that:
</p>
<ul class="org-ul">
<li>starts with a prior belief
</li>
<li>takes in some data
</li>
<li>updates the prior belief to a posterior belief.
</li>
</ul>

<p>
Thus, all Bayesian models require a prior in order to function!  
</p>
</div>
</div>

<div id="outline-container-sec-1-3" class="outline-3">
<h3 id="sec-1-3">Role of the prior?</h3>
<div class="outline-text-3" id="text-1-3">
<p>
This is where Bayesian modeling gets its most criticism.  In theory, you can use ANY prior you want.  Ideally, the prior you use should reflect your prior state of knowledge about the model.  
</p>

<p>
To see how the choice of prior can affect your posterior, consider the diagram below:
</p>


<div class="figure">
<p><img src="figures/week9/priors.jpg" alt="priors.jpg" />
</p>
</div>

<p>
In the first row, we use a <i>uniform prior</i>.  That is, each value of \(p\) is equally likely.  When we multiply the prior by the likelihood, the resulting posterior looks the same as the likelihood.
</p>

<p>
In the second row, we use a different kind of prior.  Here, our prior belief is that \(p\) MUST be larger than 0.5.  When multiplying by the likelihood, our resulting posterior belief still reflects this.  Notice that the posterior probability is still 0 for any \(p<0.5\)
</p>

<p>
Finally, in the third row, the peaked prior shifts and distorts the posterior (relative to the original likelihood).
</p>
</div>
</div>
</div>

<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2">Computations with Bayesian models</h2>
<div class="outline-text-2" id="text-2">
<p>
So far, we have concentrated on the conceptual side of Bayesian modeling.  That is, we've talked about what Bayesian models do (build posterior distributions based on priors and data).  We haven't actually talked about HOW to do these computations.  That's where we'll go next.
</p>

<p>
The mathematics behind Bayesian computation can get pretty complex.  Any course in mathematical statistics that does Bayesian computation will require knowledge of the integral calculus.
</p>

<p>
Fortunately, we now have modern computing methods that can do really good approximations for us.  As a first encounter, we'll talk about <b>grid approximation</b> today.
</p>
</div>

<div id="outline-container-sec-2-1" class="outline-3">
<h3 id="sec-2-1">Grid approximation</h3>
<div class="outline-text-3" id="text-2-1">
<p>
Grid approximation works on the basis of dividing the "parameter space" (that is, all the values of \(p\) we could consider) into a <i>finite set</i> of points.  Then, we can define our prior and likelihood on this finite set of points, after which the posterior can be computed using simple arithmetic.  
</p>

<p>
Here's how it works:
</p>

<ol class="org-ol">
<li>Define the grid. This means you decide how many points to use in estimating the posterior, and then you make a list of the parameter values on the grid.
</li>
<li>Compute the value of the prior at each parameter value on the grid.
</li>
<li>Compute the likelihood at each parameter value.
</li>
<li>Multiplying the prior by the likelihood.  This gives you the <i>unstandardized</i> posterior at each parameter value on the grid
</li>
<li>Finally, standardize the posterior (that is, turn it into a probability function).  This is done by dividing each value by the sum of all values. 
</li>
</ol>

<p>
For our globe tossing example, the following code will accomplish each of these steps:
</p>

<pre class="example">
p_grid = seq(from=0, to=1, length.out=20)
prior = rep(1, 20)
likelihood = dbinom(x=6, size=9, prob=p_grid)
posterior = likelihood * prior
posterior = posterior/sum(posterior)
</pre>

<p>
We can plot the resulting posterior distribution as follows:
</p>

<pre class="example">
plot(p_grid, posterior, type="b")
</pre>


<div class="figure">
<p><img src="figures/week9/gridApproximation.png" alt="gridApproximation.png" />
</p>
</div>

<p>
As an exercise, you should try using sparser grids (i.e., less than 20 points) and denser grids (i.e., more than 20 points).  What happens to your plot of the posterior?
</p>

<p>
Also, we can investigate the different priors we used earlier.  Re-run the code chunks above with the following definitions for <code>prior</code>:
</p>

<pre class="example">
prior = ifelse(p_grid &lt; 0.5, 0, 1)
prior = exp(-5*abs(p_grid - 0.5))
</pre>
</div>
</div>

<div id="outline-container-sec-2-2" class="outline-3">
<h3 id="sec-2-2">Using sampling to approximate posterior</h3>
<div class="outline-text-3" id="text-2-2">
<p>
In practice, most methods for computing posteriors rely on <b>sampling</b> from the posterior.  The idea is that once the model is built (i.e., once you've defined the prior and likelihood), we can estimate the posterior by pulling LOTs of samples, and then using those samples to answer questions that we care about.
</p>

<p>
To see how this works, we'll illustrate with our globe tossing model.
</p>

<p>
Let's start with a fairly dense grid approximation:
</p>

<pre class="example">
p_grid = seq(from=0, to=1, length.out=1000)
prior = rep(1, 1000)
likelihood = dbinom(x=6, size=9, prob=p_grid)
posterior = likelihood * prior
posterior = posterior/sum(posterior)
</pre>

<p>
The following code will pull 10,000 samples from the posterior distribution:
</p>

<pre class="example">
samples = sample(p_grid, prob=posterior, size=10000, replace=TRUE)
</pre>

<p>
We can see those samples here:
</p>

<pre class="example">
plot(samples)
</pre>


<div class="figure">
<p><img src="figures/week9/samples.png" alt="samples.png" />
</p>
</div>

<p>
We can also plot the density of those samples.  Notice how closely it resembles our posterior plots from above.
</p>

<pre class="example">
plot(density(samples))
</pre>


<div class="figure">
<p><img src="figures/week9/density.png" alt="density.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-sec-2-3" class="outline-3">
<h3 id="sec-2-3">Computations with posterior samples</h3>
<div class="outline-text-3" id="text-2-3">
<p>
Once we have our posterior samples, the model's work is done.  But your work as a modeler has just begun.  The next step is to <i>summarize and interpret</i> the posterior distribution. Exactly how it is summarized depends upon your purpose. But common questions include:
</p>

<ul class="org-ul">
<li>How much posterior probability lies below some parameter value?
</li>
<li>How much posterior probability lies between two parameter values?
</li>
<li>Which parameter value marks the lower 5% of the posterior probability?
</li>
<li>Which range of parameter values contains 90% of the posterior probability?
</li>
<li>Which parameter value has highest posterior probability?
</li>
</ul>

<p>
These simple questions can be usefully divided into questions about (1) intervals of defined boundaries, (2) questions about intervals of defined probability mass, and (3) questions about point estimates. We'll see how to approach these questions using samples from the posterior.
</p>
</div>

<div id="outline-container-sec-2-3-1" class="outline-4">
<h4 id="sec-2-3-1">Intervals of defined boundaries.</h4>
<div class="outline-text-4" id="text-2-3-1">
<p>
Suppose I ask you for the posterior probability that the proportion of water is less than 0.5. Using the grid-approximate posterior, you can just add up all of the samples where the corresponding parameter value is less than 0.5:
</p>

<pre class="example">
sum(samples&lt;0.5)/10000
</pre>

<p>
We can also ask what proportion of the posterior distribution is between \(p=0.5\) and \(p=0.75\).
</p>

<pre class="example">
sum(samples&gt;0.5 &amp; samples&lt;0.75)/10000
</pre>
</div>
</div>

<div id="outline-container-sec-2-3-2" class="outline-4">
<h4 id="sec-2-3-2">Intervals of defined probability mass</h4>
<div class="outline-text-4" id="text-2-3-2">
<p>
Suppose instead I ask you for the 80th percentile of the posterior distribution:
</p>

<pre class="example">
quantile(samples,0.8)
</pre>

<p>
We can also do confidence intervals (usually called <i>credible intervals</i> in Bayesian modeling).  For example, we can compute a 80% confidence interval:
</p>

<pre class="example">
quantile(samples, c(0.1,0.9))
</pre>

<p>
In contrast to confidence intervals, the bounds of a 80% <i>credible interval</i> can be interpreted in terms of probability.  That is, there is a 80% probability that \(p\) is between 0.46 and 0.82.  This is not true for confidence intervals!
</p>

<p>
There is another type of interval estimate that we can compute with posterior samples: the <b>highest posterior density interval (HPDI)</b>.  The HPDI is defined as the narrowest interval containing the specified probability mass.  It is usually different from the 80% credible interval, as we'll see in the following example.
</p>

<p>
Note: the code below will look a little more complicated.  That is because base R cannot do the HPDI computation.  However, the <code>coda</code> package can.  Since it works with MCMC samples (more on this later), we first have to convert our posterior samples to an MCMC sample.  Then, we can use the <code>HPDinterval</code> function:
</p>

<pre class="example">
library(coda)
sampMCMC = as.mcmc(samples)
HPDinterval(sampMCMC, prob=0.80)
</pre>

<p>
Compare the endpoints of the 80% HPDI with the endpoints of the 80% credible interval.
</p>
</div>
</div>

<div id="outline-container-sec-2-3-3" class="outline-4">
<h4 id="sec-2-3-3">Point estimates</h4>
<div class="outline-text-4" id="text-2-3-3">
<p>
We can also just compute a mean or median:
</p>

<pre class="example">
mean(samples)
median(samples)
</pre>

<p>
However, if the posterior distribution is skewed, a mode might be better.  The drawback is that it is a little harder.  The idea of the code below is to compute the density of the samples, then find the \(x\) value (i.e., the parameter value) that produces the highest density (<code>which.max</code>).
</p>

<pre class="example">
dens = density(samples)
dens$x[which.max(dens$y)]
</pre>
</div>
</div>
</div>
</div>



<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3">Evaluating Bayesian models</h2>
<div class="outline-text-2" id="text-3">
<p>
At this point, our computations have given us information about the plausible values of \(p\) for our binomial model.  For example, we computed an 80% HPDI for \(p\) to be [0.47,0.83], which means that \(p\) lies between 0.47 and 0.83 with probability 0.80.  In fact, we estimate the entire posterior distribution (i.e., the probability values for <b>all</b> \(p\) between 0 and 1).
</p>

<p>
We can now take this information and "go the other way".  That is, we can use the estimated parameter values \(p\) and estimate how likely a given data observation would be.  That is, our model is <i>generative</i> in the sense that we can generate predictions from the model.
</p>

<p>
To illustrate, let's assume that \(p=0.7\).  The following R code will perform 1000 simulations of our experiment (as a reminder, remember that we started with the game of tossing a globe 9 times and recording W or L..the <code>rbinom</code> will count the number of "successes" as water landings). Then, we'll plot a very simple type of histogram to show the relative frequency of each possible number of outcomes (0-9).
</p>

<pre class="example">
predictions = rbinom(1000, size=9, prob=0.7)
plot(table(predictions), xlim=c(0,9))
</pre>


<div class="figure">
<p><img src="figures/week9/predict1.jpeg" alt="predict1.jpeg" />
</p>
</div>

<p>
Note that with \(p=0.7\), we still get quite a large range of possible outcomes.  However, the outcomes \(x=6\) and \(x=7\) are still the most frequent (as we would expect, since 70% of 9 is 6.3).
</p>

<p>
As an exercise, you should play around with different values of \(p\), ranging from small values (e.g., \(p=0.1\)) to larger values (e.g., \(p=0.9\)).  What changes about the distribution of predictions?  
</p>

<p>
The R code below will show how the distribution of predictions changes for \(p\) from 0.1 to 0.9:
</p>

<pre class="example">
par(mfrow=c(1,5))
Ps = c(0.1,0.3,0.5,0.7,0.9)
for (i in 1:5){
  predictions = rbinom(1000, size=9, prob=Ps[i])
  plot(table(predictions), xlim=c(0,9), ylab="", main=paste("p=",Ps[i]))
}
par(mfrow=c(1,1))
</pre>


<div class="figure">
<p><img src="figures/week9/predictive.png" alt="predictive.png" />
</p>
</div>

<p>
However, our estimate for \(p\) is a <i>distribution</i> of values..not a single value.  Thus, we need to incorporate our uncertainty for \(p\) in our prediction as well.  The way to do this is through the <b>posterior predictive</b> distribution. 
</p>

<p>
Basically, the idea is as follows.  From the figure above, we can see that as \(p\) increases from 0 to 1, the peak of the predictive distribution shifts from the low end \(x=1\) to the high end \(x=9\).  However, from our posterior distribution of \(p\), we know that the probability that \(p\) lies on either of these ends is very small.  Thus, we need to incorporate this knowledge into our predictive distribitution.  
</p>

<p>
Essentially, we need to form a <i>weighted average</i> of predictions.  Predicted observations on the low end (\(x=0,1\)) come from small values of \(p\), which are not very common in our posterior distribution.  Similarly, predicted observations on the high end (\(x=8,9\)) come from large values of \(p\), which are also not very common in the posterior distribution.  When we weight the likelihood of the various observations by the relative posterior probabilities for \(p\), we get a result that we call the <b>posterior predictive distribution</b>.  The math can be complicated, but the R code is simple:
</p>

<pre class="example">
predictions = rbinom(1000, size=9, prob=samples)
plot(table(predictions), xlim=c(0,9))
</pre>


<div class="figure">
<p><img src="figures/week9/postPredictive.png" alt="postPredictive.png" />
</p>
</div>

<p>
Notice how the only bit of code that we changed is the <code>prob</code> value.  In the above samples, we used a fixed value of \(p\).  In the posterior predictive distribution, we set \(p\) to be a random variable that is shaped like our posterior distribution for \(p\).  Since we approximated this distribution with <code>samples</code> earlier, we can simply set <code>prob=samples</code> to accomplish this.
</p>

<p>
Notice how the posterior predictive distribution peaks around 6, which matches with our observed data (remember, we saw \(x=6\) water landings in our example).  Thus, we can conclude that the model is adequate in the sense that it predicts what we've already seen.  This predictive adequacy is a fundamental part of Bayesian modeling, and it is called a <b>posterior predictive check</b>.
</p>
</div>
</div>

<div id="outline-container-sec-4" class="outline-2">
<h2 id="sec-4">Summary</h2>
<div class="outline-text-2" id="text-4">
<p>
In this lecture, we have talked about how to construct a Bayesian model.  We begin with a <i>data story</i>; that is, we hypothesize a process by which our data might have arisen.  This results in a <i>likelihood</i> function.  We then decide on a prior, which represents our prior knowledge. Then, we use Bayes theorem to update our model by <i>feeding</i> it the data. Technically, this amounts to computing a <i>posterior</i> by multiplying the prior and the likelihood.  Computationally, we use <i>sampling</i> to accomplish this.  Once we estimate our posterior distribution, we check our model by generating a <i>posterior predictive distribution</i>, which incorporates both data uncertainty as well as parameter uncertainty into a single distribution.  We compare this posterior predictive distribution to our observed data; if the observed data match what we would <i>expect</i> from the model (based on the posterior predictive), we can conclude that our model does an adequate job of describing our data.
</p>

<p>
Next time, we will focus on more modern method of posterior sampling known as <b>Markov chain Monte Carlo</b> sampling. 
</p>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: October 23, 2017</p>
<p class="date">Created: 2017-10-23 Mon 09:47</p>
<p class="creator"><a href="http://www.gnu.org/software/emacs/">Emacs</a> 25.2.1 (<a href="http://orgmode.org">Org</a> mode 8.2.10)</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
