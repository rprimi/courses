<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>Week 8 lecture notes - PSYC 5316</title>
<!-- 2017-10-15 Sun 15:02 -->
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="generator" content="Org-mode" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center; }
  .todo   { font-family: monospace; color: red; }
  .done   { color: green; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.right  { text-align: center;  }
  th.left   { text-align: center;   }
  th.center { text-align: center; }
  td.right  { text-align: right;  }
  td.left   { text-align: left;   }
  td.center { text-align: center; }
  dt { font-weight: bold; }
  .footpara:nth-child(2) { display: inline; }
  .footpara { display: block; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/javascript" src="http://orgmode.org/mathjax/MathJax.js"></script>
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
    MathJax.Hub.Config({
        // Only one of the two following lines, depending on user settings
        // First allows browser-native MathML display, second forces HTML/CSS
        //  config: ["MMLorHTML.js"], jax: ["input/TeX"],
            jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        showProcessingMessages: true,
        displayAlign: "center",
        displayIndent: "2em",

        "HTML-CSS": {
             scale: 100,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<h1 class="title">Week 8 lecture notes - PSYC 5316</h1>

<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1">Introduction to Bayesian inference</h2>
<div class="outline-text-2" id="text-1">
<p>
Bayesian modeling represents a fundamentally different approach to inference.  Mathematically, it is based on something called <b>Bayes' Theorem</b>.  Today we will illustrate how Bayes Theorem relates to things we actually care about (probabilities!), and then we will begin using these concepts to construct <b>data models</b> to use for inference.
</p>
</div>
</div>

<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2">Logic of hypothesis testing</h2>
<div class="outline-text-2" id="text-2">
<p>
Suppose you have a treatment that you suspect may alter performance on a certain task. You  nd that the experimental group signi cantly outperforms the control group, \(t(18) = 2.7\), \(p = 0.01\).
</p>

<p>
How many of the following are True statements?
</p>

<ol class="org-ol">
<li>The probability that the null hypothesis is true is 1%
</li>
<li>The probability that the research hypothesis is true is 99%
</li>
<li>If you reject the null, the probability that you are making the wrong decision is 1%
</li>
<li>If you repeated the experiment over and over, then you would get a significant result 99% of the time.
</li>
</ol>

<p>
Turns out they're all false!  So what does \(p=0.01\) mean, anyway?
</p>

<p>
Recall that the <b>p-value</b> is the probability of obtaining a test statistic <i>at least as extreme</i> as the one that was observed, <b>given that the null hypothesis is true</b>.
</p>

<p>
In symbols, this can be written \(P(D\mid \mathcal{H}_0)\)
</p>

<p>
Thus, the core logic of frequentist hypothesis testing is:
</p>
<ul class="org-ul">
<li>suppose the opposite of what you want
</li>
<li>show that this hypothesis is very unlikelly (i.e., low \(p\)-value)
</li>
</ul>

<p>
This is similar to the <b>modus tollens</b> syllogism from formal logic:
</p>

<ul class="org-ul">
<li>Premise:  If A, then B;
</li>
<li>Premise:  not B;
</li>
<li>Conclusion: therefore, not A
</li>
</ul>

<p>
Example:
</p>
<ul class="org-ul">
<li>If Kelsey is happy, then she is smiling
</li>
<li>Kelsey is not smiling.
</li>
<li>Therefore, Kelsey is not happy.
</li>
</ul>

<p>
This maps onto hypothesis testing in the following way (this is called Fisher's disjunction):
</p>

<ul class="org-ul">
<li>If \(\mathcal{H}_0\), then we shouldn't observe \(D\)
</li>
<li>we observed \(D\)
</li>
<li>therefore, not \(\mathcal{H}_0\)
</li>
</ul>

<p>
However, hypothesis testing uses a <b>probabilistic</b> version of this:
</p>

<ul class="org-ul">
<li>If \(\mathcal{H}_0\), then observing \(D\) is <i>very unlikely</i>
</li>
<li>we observed \(D\)
</li>
<li>therefore, \(\mathcal{H}_0\) is very unlikely
</li>
</ul>

<p>
The problem is that this is not a logically valid argument!
</p>

<p>
Example:
</p>
<ul class="org-ul">
<li>If an individual is a man, then he is unlikely to be Pope
</li>
<li>Francis is the pope
</li>
<li>therefore, Francis is probably not a man
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3">Bayes' Theorem</h2>
<div class="outline-text-2" id="text-3">
<p>
The previous discussion leaves us with the following question:
</p>
<ul class="org-ul">
<li>If \(p(D\mid \mathcal{H}_0)\) is small,
</li>
<li>and we observe \(D\)
</li>
<li>what can we logically say about \(p(\mathcal{H}_0\mid D)\)?
</li>
</ul>

<p>
The answer comes from something called <i>Bayes' Theorem</i>:
</p>

<p>
\[
p(\mathcal{H}_0\mid D) = \frac{p(D\mid \mathcal{H}_0) \cdot p(\mathcal{H}_0)}{p(D)}
\]
</p>

<p>
We usually remember Bayes' theorem using the following words:
</p>

<p>
\[
\text{posterior} = \frac{\text{likelihood} \times \text{prior}}{\text{average likelihood}}
\]
</p>

<p>
Since the denominator is just a scaling factor (to make it all sum to 1; that is, an <i>actual</i> probability function), we usually just remember the following:
</p>

<p>
\[
\text{posterior} \propto \text{likelihood}\times \text{prior}
\]
</p>
</div>
</div>

<div id="outline-container-sec-4" class="outline-2">
<h2 id="sec-4">An example from Harry Potter</h2>
<div class="outline-text-2" id="text-4">
<p>
At Hogwarts, Professor Pomona Sprout leads the Herbology Department. In the Department’s greenhouses, she cultivates crops of a magical plant called green codacle – a flowering plant that when consumed causes a witch or wizard to feel euphoric and relaxed. Professor Sybill Trelawney, the Professor of Divination, is an avid user of green codacle and frequently visits Professor Sprout’s laboratory to sample the latest harvest.
</p>

<p>
However, it has turned out that one in a thousand codacle plants is a afflicted with a mutation that changes its effects – consuming one of those rare plants causes unpleasant side effects, such as paranoia, anxiety, and spontaneous levitation.
</p>

<p>
In order to evaluate the quality of her crops, Professor Sprout has developed a mutation-detecting spell. The new spell has a 99% chance to accurately detect an existing mutation, but also has a 2% chance to falsely indicate that a healthy plant is a mutant.
</p>

<p>
When Professor Sprout presents her results at a School colloquium, Trelawney, trying to discern how much trust to put in Sprout’s spell, asks "What is the probability that a codacle plant is a mutant, when your spell says that it is?"
</p>

<p>
Let's think about what we know:
</p>
<ul class="org-ul">
<li>Professor Sprout’s mutation spell has a 99% chance of correctly detecting that a given codacle plant is a mutant
</li>
<li>There is a 2% chance that the spell will falsely indicate that a healthy plant is a mutant.
</li>
<li>Mutations occur at a rate of 1 in a 1000
</li>
</ul>

<p>
Let's also define some notation to make our lives a bit easier:
</p>

<ul class="org-ul">
<li>\(\mathcal{H}_0\): the hypothesis that the plant is NOT a mutant
</li>
<li>\(\mathcal{H}_1\): the hypothesis that the plant IS a mutant
</li>
<li>\(D\): an observation (data) that Sprout's spell detects "mutant"
</li>
<li>\(-D\): an observation (data) that Sprout's spell detects "not mutant"
</li>
</ul>


<div class="figure">
<p><img src="figures/week8/decisionTree.png" alt="decisionTree.png" />
</p>
</div>

<p>
Based on this tree, we can compute:
</p>

\begin{align*}
p(\mathcal{H_1}\mid D) & = \frac{p(D\mid \mathcal{H}_1)\cdot p(\mathcal{H}_1)}{p(D)}\\
&= \frac{0.99\cdot 0.001}{0.00099+0.01998}\\
&= 0.047
\end{align*}

<p>
Thus, even with VERY high accuracy, the very low prior probability of being a mutant makes it so that our posterior probability is only 4.7% 
</p>
</div>
</div>

<div id="outline-container-sec-5" class="outline-2">
<h2 id="sec-5">Building Bayesian models</h2>
<div class="outline-text-2" id="text-5">
<p>
Bayesian modeling uses the basic vocabulary of Bayes theorem.  Our goal is to quantify our <b>posterior beliefs</b> in a model.  We do this by <b>updating</b> our <b>prior beliefs</b> via Bayes theorem&#x2026;that is, posterior = prior x likelihood.
</p>

<p>
Let's revisit the "globe tossing" example from the first exam.
</p>

<p>
Suppose you have a globe that represents the Earth.  You would like to estimate how much of the surface is covered in water.  To measure this, you adopt the following strategy: toss the globe up in the air. When you catch it, you will record whether the surface under your right index finger is water or land.  Then you toss the globe up in the air again and repeat the procedure. The first nine samples generate the following sequence:
</p>

<p>
W L W W W L W L W
</p>

<p>
where W indicates water and L indicates land. So in this example you observe six W (water) observations and three L (land) observations. Call this sequence of observations the <b>data</b>.
</p>

<p>
To construct a model, we need to make assumptions.  Designing a simple Bayesian model uses a design loop with three steps.
</p>

<ol class="org-ol">
<li>Data story: Motivate the model by narrating how the data might arise.
</li>
<li>Update: Educate your model by feeding it the data.
</li>
<li>Evaluate: All statistical models require supervision, leading possibly to model revision.  We'll talk about this later.
</li>
</ol>
</div>

<div id="outline-container-sec-5-1" class="outline-3">
<h3 id="sec-5-1">The data story</h3>
<div class="outline-text-3" id="text-5-1">
<p>
The "data story" amounts to explaining how each piece of data is born. This usually means describing aspects of the underlying reality as well as the sampling process. The data story in this case is simply a restatement of the sampling process:
</p>
<ul class="org-ul">
<li>Assume the true proportion of water covering the globe is p.
</li>
<li>A single toss of the globe has a probability p of producing a water (W) observation. It has a probability 1 − p of producing a land (L) observation.
</li>
<li>Each toss of the globe is independent of the others.
</li>
</ul>

<p>
This is usually translated into a probability statement, called a <b>likelihood</b>.  Based on the story, we can use a <b>binomial</b> likelihood to model the data generation.
</p>

<p>
\[
f(x,N,p) = \binom{N}{x}p^{x}(1-p)^{1-x}
\]
</p>
</div>
</div>

<div id="outline-container-sec-5-2" class="outline-3">
<h3 id="sec-5-2">Updating</h3>
<div class="outline-text-3" id="text-5-2">

<div class="figure">
<p><img src="figures/week8/globeTossing.jpg" alt="globeTossing.jpg" />
</p>
</div>

<p>
In this figure, we see how Bayesian updating works.  
</p>
<ul class="org-ul">
<li>start with a prior that assigns equal probability to all values of \(p\) between 0 and 1.
</li>
<li>after seeing the first "W", our posterior changes.  Now, \(p=0\) is impossible (probability = 0), and \(p>0.5\) is much more likely than \(p<0.5\).
</li>
<li>after seeing the next data "L", the posterior changes again.  Both \(p=0\) and \(p=1\) are impossible, with \(p=0.5\) most likely (which makes sense, since out of TWO tosses, we've seen one W and one L).
</li>
<li>after seeing the next data "W", the posterior again shifts toward \(p=1\), but note that \(p=1\) is still impossible.
</li>
<li>each observation of "W" shifts the peak toward the right \(p=1\), while each observation of "L" shifts the peak toward the left.
</li>
</ul>

<p>
From this, we can think of a Bayesian model as a machine that:
</p>
<ul class="org-ul">
<li>starts with a prior belief
</li>
<li>takes in some data
</li>
<li>updates the prior belief to a posterior belief.
</li>
</ul>

<p>
Thus, all Bayesian models require a prior in order to function!  
</p>
</div>
</div>

<div id="outline-container-sec-5-3" class="outline-3">
<h3 id="sec-5-3">Role of the prior?</h3>
<div class="outline-text-3" id="text-5-3">
<p>
This is where Bayesian modeling gets its most criticism.  In theory, you can use ANY prior you want.  Ideally, the prior you use should reflect your prior state of knowledge about the model.  
</p>

<p>
To see how the choice of prior can affect your posterior, consider the diagram below:
</p>


<div class="figure">
<p><img src="figures/week8/priors.jpg" alt="priors.jpg" />
</p>
</div>

<p>
In the first row, we use a <i>uniform prior</i>.  That is, each value of \(p\) is equally likely.  When we multiply the prior by the likelihood, the resulting posterior looks the same as the likelihood.
</p>

<p>
In the second row, we use a different kind of prior.  Here, our prior belief is that \(p\) MUST be larger than 0.5.  When multiplying by the likelihood, our resulting posterior belief still reflects this.  Notice that the posterior probability is still 0 for any \(p<0.5\)
</p>

<p>
Finally, in the third row, the peaked prior shifts and distorts the posterior (relative to the original likelihood).
</p>
</div>
</div>
</div>

<div id="outline-container-sec-6" class="outline-2">
<h2 id="sec-6">Computations with Bayesian models</h2>
<div class="outline-text-2" id="text-6">
<p>
So far, we have concentrated on the conceptual side of Bayesian modeling.  That is, we've talked about what Bayesian models do (build posterior distributions based on priors and data).  We haven't actually talked about HOW to do these computations.  That's where we'll go next.
</p>

<p>
The mathematics behind Bayesian computation can get pretty complex.  Any course in mathematical statistics that does Bayesian computation will require knowledge of the integral calculus.
</p>

<p>
Fortunately, we now have modern computing methods that can do really good approximations for us.  As a first encounter, we'll talk about <b>grid approximation</b> today.
</p>
</div>

<div id="outline-container-sec-6-1" class="outline-3">
<h3 id="sec-6-1">Grid approximation</h3>
<div class="outline-text-3" id="text-6-1">
<p>
Grid approximation works on the basis of dividing the "parameter space" (that is, all the values of \(p\) we could consider) into a <i>finite set</i> of points.  Then, we can define our prior and likelihood on this finite set of points, after which the posterior can be computed using simple arithmetic.  
</p>

<p>
Here's how it works:
</p>

<ol class="org-ol">
<li>Define the grid. This means you decide how many points to use in estimating the posterior, and then you make a list of the parameter values on the grid.
</li>
<li>Compute the value of the prior at each parameter value on the grid.
</li>
<li>Compute the likelihood at each parameter value.
</li>
<li>Multiplying the prior by the likelihood.  This gives you the <i>unstandardized</i> posterior at each parameter value on the grid
</li>
<li>Finally, standardize the posterior (that is, turn it into a probability function).  This is done by dividing each value by the sum of all values. 
</li>
</ol>

<p>
For our globe tossing example, the following code will accomplish each of these steps:
</p>

<pre class="example">
p_grid = seq(from=0, to=1, length.out=20)
prior = rep(1, 20)
likelihood = dbinom(x=6, size=9, prob=p_grid)
posterior = likelihood * prior
posterior = posterior/sum(posterior)
</pre>

<p>
We can plot the resulting posterior distribution as follows:
</p>

<pre class="example">
plot(p_grid, posterior, type="b")
</pre>


<div class="figure">
<p><img src="figures/week8/gridApproximation.png" alt="gridApproximation.png" />
</p>
</div>

<p>
As an exercise, you should try using sparser grids (i.e., less than 20 points) and denser grids (i.e., more than 20 points).  What happens to your plot of the posterior?
</p>

<p>
Also, we can investigate the different priors we used earlier.  Re-run the code chunks above with the following definitions for <code>prior</code>:
</p>

<pre class="example">
prior = ifelse(p_grid &lt; 0.5, 0, 1)
prior = exp(-5*abs(p_grid - 0.5))
</pre>
</div>
</div>

<div id="outline-container-sec-6-2" class="outline-3">
<h3 id="sec-6-2">Using sampling to approximate posterior</h3>
<div class="outline-text-3" id="text-6-2">
<p>
The methods we'll use this semester rely on <b>sampling</b> from the posterior.  The idea is that once the model is built, we can do computations with the posterior by pulling LOTs of samples, and then using those samples to answer questions.
</p>

<p>
To see how this works, we'll illustrate with our globe tossing model.
</p>

<p>
Let's start with a fairly dense grid approximation:
</p>

<pre class="example">
p_grid = seq(from=0, to=1, length.out=1000)
prior = rep(1, 1000)
likelihood = dbinom(x=6, size=9, prob=p_grid)
posterior = likelihood * prior
posterior = posterior/sum(posterior)
</pre>

<p>
The following code will pull 10,000 samples from the posterior distribution:
</p>

<pre class="example">
samples = sample(p_grid, prob=posterior, size=10000, replace=TRUE)
</pre>

<p>
We can see those samples here:
</p>

<pre class="example">
plot(samples)
</pre>


<div class="figure">
<p><img src="figures/week8/samples.png" alt="samples.png" />
</p>
</div>

<p>
We can also plot the density of those samples.  Notice how closely it resembles our posterior plots from above.
</p>

<pre class="example">
plot(density(samples))
</pre>


<div class="figure">
<p><img src="figures/week8/density.png" alt="density.png" />
</p>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: October 16, 2017</p>
<p class="date">Created: 2017-10-15 Sun 15:02</p>
<p class="creator"><a href="http://www.gnu.org/software/emacs/">Emacs</a> 25.2.1 (<a href="http://orgmode.org">Org</a> mode 8.2.10)</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
