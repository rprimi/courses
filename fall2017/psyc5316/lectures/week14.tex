% Created 2017-11-27 Mon 18:06
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fixltx2e}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{marvosym}
\usepackage{wasysym}
\usepackage{amssymb}
\usepackage{hyperref}
\tolerance=1000
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}
\usepackage{amsmath}
\date{November 27, 2017}
\title{Week 14 lecture notes - PSYC 5316}
\hypersetup{
  pdfkeywords={},
  pdfsubject={},
  pdfcreator={Emacs 25.3.1 (Org mode 8.2.10)}}
\begin{document}

\maketitle
In our final lecture of the semester, we will discuss \emph{Bayes factors}, an alternative to traditional null hypothesis testing.

\section*{Definition}
\label{sec-1}

A natural consequence of Bayes Theorem provides a nice framework for testing hypotheses.  Let $H_0$ denote the null hypothesis model, and let $H_1$ denote the alternative model.  As a Bayesian, we are interested in knowing $P(H_i\mid D)$, where $D=\mathrm{data}$ and $i=0,1$.

From Bayes Theorem, we get the following:

\[
P(H_1\mid D) = \frac{P(D\mid H_1) \cdot P(H_1)}{P(D)}
\]

and

\[
P(H_0\mid D) = \frac{P(D\mid H_0) \cdot P(H_0)}{P(D)}.
\]

Taking the ratio, we then can write:

\begin{align*}
\frac{P(H_1\mid D)}{P(H_0\mid D)} &= \frac{\frac{P(D\mid H_1) \cdot P(H_1)}{P(D)}}{\frac{P(D\mid H_0) \cdot P(H_0)}{P(D)}}\\
&= \frac{P(D\mid H_1) \cdot P(H_1)}{P(D)} \cdot \frac{P(D)}{P(D\mid H_0)\cdot P(H_0)} \\
&= \frac{P(D\mid H_1) \cdot P(H_1)}{P(D\mid H_0)\cdot P(H_0)}\\
\end{align*}

We can rewrite this slightly more informatively as:

\[
\underbrace{\frac{p(H_1 \mid D)}{p(H_0\mid D)}}_{\text{posterior odds}} = \underbrace{\frac{p(D\mid H_1)}{p(D\mid H_0)}}_{\text{Bayes factor}} \cdot \underbrace{\frac{p(H_1)}{p(H_0)}}_{\text{prior odds}}
\]

The term in the middle, which Kass and Raftery (1995) called the \emph{Bayes Factor}, tells us how much our observation of data updates the prior odds of $H_1$ over $H_0$.  In other words, it tells us how \emph{informative} our data is.

Jeffreys (1961) calls this \emph{evidence}, and gives recommendations of how Bayes factors relate to evidence.

\begin{center}
\begin{tabular}{rl}
BF & evidence\\
\hline
1 & no evidence\\
1-3 & anecdotal evidence\\
3-10 & moderate evidence\\
10-30 & strong evidence\\
30-100 & very strong evidence\\
 & \\
\end{tabular}
\end{center}

\section*{Bayesian t-tests}
\label{sec-2}

Though Bayes factors have been around a while, they are pretty new to our field.  The first substantive work on applying Bayes factors to our basic inferential tools was by Jeff Rouder and Richard Morey, whose first paper on Bayesian t-tests came out in 2009!  Thankfully, Richard Morey has developed a very nice R package for computing Bayes factors in common designs (t-tests, ANOVAs, etc.).  We will demonstrate this package and how to interpret Bayes factors tonight.

\subsection*{paired samples t-test}
\label{sec-2-1}

To begin our work, we need to load two packages:

\begin{verbatim}
library(BayesFactor)
library(tidyverse)
\end{verbatim}

Next, we will load a data set that is included in the \texttt{BayesFactor} package. This data set shows the effect of two soporific drugs on a sample of 10 patients.  The variable \texttt{extra} represents the "extra sleep" that each patient got after taking the drug.

\begin{verbatim}
data(sleep)
\end{verbatim}

Since each patient took two drugs (denoted by the variable \texttt{ID}), we might be interested in whether there is a difference in the amount of extra sleep gained between the two drugs.  This calls for a paired samples t-test.

Remember that to perform a paired samples t-test, one first computes \emph{difference scores}, and then tests these difference scores against a (null) hypothesized difference score of 0.

First, let's do some data wrangling to compute our difference scores:

\begin{verbatim}
drug1 = sleep %>%
  filter(group==1)
drug2 = sleep %>%
  filter(group==2)

change = drug2$extra = drug1$extra
\end{verbatim}

Now \texttt{change} is a vector of 10 difference scores.  We can easily compute mean and standard deviation of these 10 difference scores, and from this we can get an estimate of the effect size:

\begin{verbatim}
mean(change)
sd(change)
mean(change)/sd(change)
\end{verbatim}

From this, we see that the mean difference score is 1.58, with a standard deviation of 1.23.  Thus, the overall (standardized) effect size is 1.58/1.23 = 1.28, which we consider a large effect.

The traditional t-test proceeds as follows:

\begin{verbatim}
t.test(change, mu=0)
\end{verbatim}

From the output, we see that we reject the null and conclude that there is a significant difference between the two drugs, $t(9)=4.06$, $p=0.003$.

For the Bayesian version of the t-test, we do the following:

\begin{verbatim}
bf = ttestBF(x=change)
bf
\end{verbatim}

The output looks like this:

\begin{verbatim}
Bayes factor analysis
--------------
[1] Alt., r=0.707 : 17.25888 ±0%

Against denominator:
  Null, mu = 0 
---
Bayes factor type: BFoneSample, JZS
\end{verbatim}

There is a lot here, but first let's focus on the primary output.  The Bayes Factor is 17.3.  This means that after seeing the data, the alternative model (i.e., $H_1$) is 17.3 times more likely than the null model ($H_0$).  According to the recommendations of Jeffreys (1961), this is strong evidence in favor of the alternative.

One of the nice things about Bayes factors is that they are simple mathematical objects that you can do things with, like taking reciprocals.

\begin{verbatim}
1/bf
\end{verbatim}

The output looks like this:

\begin{verbatim}
Bayes factor analysis
--------------
[1] Null, mu=0 : 0.05794119 ±0%

Against denominator:
  Alternative, r = 0.707106781186548, mu =/= 0 
---
Bayes factor type: BFoneSample, JZS
\end{verbatim}

This says that after seeing the data, the null is 0.06 times "more likely" than the alternative.  In other words, it is MUCH LESS LIKELY than the alternative.  However, this means that it is equally fine to measure evidence FOR the null as it is to measure evidence AGAINST the null.  This is a major limitation in null hypothesis testing that a Bayesian framework overcomes quite easily.

Now, since the Bayes factor is inherently a Bayesian object, that means we can do all of the typical parameter estimation stuff we did earlier, like posterior sampling, etc.

\begin{verbatim}
chains = posterior(bf, iterations=10000)
summary(chains)
\end{verbatim}

The output looks like this:

\begin{verbatim}
Iterations = 1:10000
Thinning interval = 1 
Number of chains = 1 
Sample size per chain = 10000 

1. Empirical mean and standard deviation for each variable,
   plus standard error of the mean:

       Mean      SD Naive SE Time-series SE
mu    1.409  0.4557 0.004557       0.005158
sig2  2.043  1.2100 0.012100       0.015412
delta 1.094  0.4414 0.004414       0.005725
g     7.465 69.1904 0.691904       0.691904

2. Quantiles for each variable:

        2.5%    25%   50%   75%  97.5%
mu    0.4751 1.1213 1.421 1.703  2.277
sig2  0.7370 1.2771 1.743 2.436  5.170
delta 0.2780 0.7868 1.084 1.387  1.997
g     0.1625 0.5464 1.234 3.157 32.005
\end{verbatim}

Notice that we get an array of estimates for several quantities, including \texttt{mu} (population mean), \texttt{sig2} (population variance), and \texttt{delta} (effect size).  Since this is Bayesian technique, we also get estimates of error in each of these quantities.  This is easily seen in the following plot:

\begin{verbatim}
plot(chains[,1:3])
\end{verbatim}

\includegraphics[width=.9\linewidth]{figures/week14/plot1.png}

On the left, we see the plot of 10000 posterior samples (notice they look like fat hairy caterpillars, which is a good thing!).  On the right, we get density plots for the posterior samples. Notice particuarly that the plots for mu and delta peak close to our original estimates of the mean difference score (1.58 and 1.28, respectively).

\subsection*{What is r?}
\label{sec-2-2}

\emph{(Note: for this section, you need a command from the \texttt{polspline} package).  If you do not already have it installed, execute \texttt{install.packages("polspline")} from the console.}

You may be wondering what the \texttt{r=0.707} in the output means?  This has to do with the \emph{prior}, which you may notice we didn't mention anything about.  Well, this is a Bayesian computation, and there has to be a prior somewhere!

The prior we use is based on the recommendations of Rouder et al. (2009). The way Rouder defines the Bayesian t-test is that we compute an estimate for the \emph{effect size} $\delta$.  The prior for the null model $H_0$ is a Cauchy prior with \emph{scale} $r$.  The Cauchy distribution is similar to the normal distribution, but it is more peaked around 0.  That is, it places more prior probability on an effect size of 0 than the normal distribution.  You can see that here:

\begin{verbatim}
x=seq(-3,3,0.01)
plot(x, dcauchy(x, scale=0.707), type="l")
lines(x, dnorm(x, mean=0, sd=1), lty=2)
\end{verbatim}

\includegraphics[width=.9\linewidth]{figures/week14/cauchy.png}

As you can see in the code, we used \texttt{scale=0.707} as an argument to plot the Cauchy density.  We could change this however we want.  However, Rouder et al. (2009) recommend using a scale of $r=\sqrt{2}/2$, which is about $r=0.707$.  I certainly invite you to play around with the plot as you use various values of \texttt{r}.

Now lets have some fun.  We know our prior on effect size, which is \texttt{dcauchy(x, scale=0.707)}.  We also have a posterior density for effect size, which we can plot as follows:

\begin{verbatim}
plot(density(chains[,3]), xlim=c(-3,3))
lines(x,dcauchy(x, scale=.707), lty=2)
\end{verbatim}

\includegraphics[width=.9\linewidth]{figures/week14/priorPosterior.png}

Notice how \emph{prior} to seeing data, we have a lot of mass on an effect size of 0.  However, \emph{after} seeing data (i.e., posterior), that mass is shifted away from 0.  How much, you might ask?  Look at the following:

\begin{verbatim}
library(polspline)
fit.posterior = logspline(chains[,3])
posterior = dlogspline(0, fit.posterior)
prior = dcauchy(0, scale=0.707)

points(0, posterior, pch=19)
points(0, prior, pch=19)
\end{verbatim}

\includegraphics[width=.9\linewidth]{figures/week14/savageDickey.png}

The ratio of the heights of these dots is exactly the Bayes factor!  Intuitively, it represents how our belief in a particular effect size (namely $\delta=0$) changes after seeing the data.  In this case, our belief in $\delta=0$ decreases by a factor of approximately 18 (to see this, compute \texttt{prior/posterior} in the R console).

By the way, this computation is called the \emph{Savage-Dickey density ratio}, and there is a nice article by E.J. Wagenmakers in the Journal of Cognitive Psychology that explains its use in psychology.
% Emacs 25.3.1 (Org mode 8.2.10)
\end{document}