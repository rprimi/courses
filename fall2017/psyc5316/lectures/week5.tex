% Created 2017-09-25 Mon 10:50
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fixltx2e}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{marvosym}
\usepackage{wasysym}
\usepackage{amssymb}
\usepackage{hyperref}
\tolerance=1000
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}
\usepackage{amsmath}
\date{September 25, 2017}
\title{Week 5 lecture notes - PSYC 5316}
\hypersetup{
  pdfkeywords={},
  pdfsubject={},
  pdfcreator={Emacs 25.2.1 (Org mode 8.2.10)}}
\begin{document}

\maketitle

\section*{Introduction}
\label{sec-1}
In this lecture, we will learn how to work with large data sets.  Data sets are commonly represented as \emph{data frames}.  To begin, we'll use the \texttt{read\_csv} function to load a data frame.  First, load the \texttt{tidyverse} package, and then execute the \texttt{read\_csv} commmand.

\begin{verbatim}
library(tidyverse)
rawdata = read_csv("https://git.io/vdkRH")
\end{verbatim}

(Note: if you haven't already installed \texttt{tidyverse}, you can do so by executing \texttt{install.packages("tidyverse")} at the console.

This data set comes from Experiment 2 of Faulkenberry, Cruise, Lavro, and Shaki (2016).  In this experiment, we presented people with pairs of single-digit numbers written in two different font sizes. We asked them to use a computer mouse to click on the \emph{physically larger} of the pair.  This resulted in experimental conditions: \emph{congruent} trials, where the physically larger number was also numerically larger, and \emph{incongruent} trials, where the physically larger number was numerically smaller.  We measured RT in milliseconds.

The data file is structured with the following columns:

\begin{center}
\begin{tabular}{ll}
column & description\\
\hline
subject & number representing subject identifier\\
condition & size-congruity condition: either "congruent" or "incongruent"\\
distance & numerical distance between presented numbers (1,2,3, or 4)\\
error & indicates whether error was made on trial: 0 = no error, 1=error\\
RT & response time in milliseconds\\
side & side of display on which the physically larger number was presented (either left or right)\\
 & \\
\end{tabular}
\end{center}
\section*{What are data frames?}
\label{sec-2}
Data frames are the de facto data structure for most tabular data, and what we use for statistics and plotting.

A data frame is the representation of data in the format of a table where the columns are vectors that all have the same length. Because the column are vectors, they all contain the same type of data (e.g., characters, integers, factors).

We can see this when inspecting the structure of our data frame with the function \texttt{glimpse}:

\begin{verbatim}
glimpse(rawdata)
\end{verbatim}

In addition to \texttt{glimpse}, there are some other functions that will help you to get a sense of the content/structure of your data:

\begin{center}
\begin{tabular}{ll}
Size & \\
\texttt{dim} & returns a vector with the number of rows in the first element, and the number of columns as the second element\\
\texttt{nrow} & returns the number of rows\\
\texttt{ncol} & returns the number of columns\\
Content & \\
\texttt{head} & shows the first 6 rows\\
\texttt{tail} & shows the last 6 rows\\
\texttt{names} & returns the column names\\
\end{tabular}
\end{center}


\section*{Data manipulation using \texttt{tidyr} and \texttt{dplyr}}
\label{sec-3}

\texttt{dplyr} is a package for making tabular data manipulation easier. It pairs nicely with \texttt{tidyr} which enables you to swiftly convert between different data formats for plotting and analysis.

The package \texttt{dplyr} provides easy tools for the most common data manipulation tasks. It is built to work directly with data frames, with many common tasks optimized by being written in a compiled language (C++). An additional feature is the ability to work directly with data stored in an external database. The benefits of doing this are that the data can be managed natively in a relational database, queries can be conducted on that database, and only the results of the query are returned.

This addresses a common problem with R in that all operations are conducted in-memory and thus the amount of data you can work with is limited by available memory. The database connections essentially remove that limitation in that you can have a database of many 100s GB, conduct queries on it directly, and pull back into R only what you need for analysis.

The package \texttt{tidyr} addresses the common problem of wanting to reshape your data for plotting and use by different R functions. Sometimes we want data sets where we have one row per measurement. Sometimes we want a data frame where each measurement type has its own column, and rows are instead more aggregated groups. Moving back and forth between these formats is nontrivial, and \texttt{tidyr} gives you tools for this and more sophisticated data manipulation.

\subsection*{Selecting columns and filtering rows}
\label{sec-3-1}

We're going to learn some of the most common \texttt{dplyr} functions (also called \textbf{verbs}): \texttt{select()}, \texttt{filter()}, \texttt{mutate()}, \texttt{group\_by()}, and \texttt{summarize()}. 

To choose \emph{columns} of a data frame, use \texttt{select()}. The first argument to this function is the data frame (\texttt{rawdata}), and the subsequent arguments are the columns to keep.

\begin{verbatim}
select(rawdata, condition, error, RT)
\end{verbatim}

To choose \emph{rows} based on specific criteria, use \texttt{filter()}

\begin{verbatim}
filter(rawdata, error==0)
\end{verbatim}

\subsection*{Pipes}
\label{sec-3-2}
But what if you wanted to select and filter at the same time?  There are many ways to do this, but the quickest and easiest is to use \textbf{pipes}.

Pipes are a fairly recent addition to R. Pipes let you take the output of one function and send it directly to the next, which is useful when you need to do many things to the same dataset. Pipes in R look like \texttt{\%>\%} and are made available via the \texttt{magrittr} package, installed automatically with \texttt{dplyr}. 

\begin{verbatim}
rawdata %>%
  filter(error==0) %>%
  select(subject, condition, RT)
\end{verbatim}

In the above code chunk, we use the pipe to send the \texttt{rawdata} dataset first through \texttt{filter()} to keep only the rows where \texttt{error=0}.  Then, we pass that output through the \texttt{select()} function to keep only the columns \texttt{subject}, \texttt{condition}, and \texttt{RT}.

Note that using the pipe operator \texttt{\%>\%} removes the need to specify the dataset as the first argument to the \texttt{filter()} or \texttt{select()} functions.

If we wanted to create a new object of this smaller version of the data, we could do so by using the assignment operator \texttt{<-}:

\begin{verbatim}
data <- rawdata %>%
  filter(error==0)
\end{verbatim}

If you look in the "Environment" tab, you'll notice that the row dimension has reduced to 15,686.  This tells us that there were 44 errors across all participants and trials.


\subsection*{Mutate}
\label{sec-3-3}
Frequently you will want to create new columns based on the values in existing columns.  For example, we might want to express response time (RT) in seconds rather than milliseconds.  For this, we'll use \texttt{mutate()}:

\begin{verbatim}
data %>%
  mutate(RT_sec = RT/1000)
\end{verbatim}

Another good use of \texttt{mutate()} is for recoding a variable.  For example, the column \texttt{distance} currently has 4 values: 1, 2, 3, and 4.  Suppose we want to recode this variable to have two values: close (distance=1 or 2) and far (distance=3 or 4).  We can use \texttt{mutate()} along with \texttt{ifelse()} to do this:

\begin{verbatim}
data %>%
  mutate(dist = ifelse(distance==1 | distance==2, "close", "far"))
\end{verbatim}

\subsection*{Summarize}
\label{sec-3-4}

Many data analysis tasks can be approached using the \emph{split-apply-combine} paradigm: split the data into groups, apply some analysis to each group, then combine the results.  \texttt{dplyr} makes this very easy using the functions \texttt{group\_by()} and \texttt{summarize()}.

The following code chunk illustrates this:

\begin{verbatim}
data %>%
  group_by(condition) %>%
  summarize(meanRT = mean(RT))
\end{verbatim}

\texttt{group\_by()} takes as its argument the column name(s) that contain the categorical variables for which you want to calculate a summary statistic (e.g., mean).  In this example, we are interested in the mean RT by condition.  You'll notice that incongruent trials take slightly longer than congruent trials.  This is the \emph{size-congruity} effect, first demonstrated by Henik and Tzelgov in 1982.

Note that you can also group by multiple columns.  Suppose we were interested in the mean RT by condition AND distance.

\begin{verbatim}
data %>%
  group_by(condition, distance) %>%
  summarize(meanRT = mean(RT))
\end{verbatim}

Also, we can compute multiple statistics:

\begin{verbatim}
data %>%
  group_by(condition, distance) %>%
  summarize(meanRT=mean(RT), sd=sd(RT))
\end{verbatim}

\section*{Visualizing data with \texttt{ggplot2}}
\label{sec-4}

\texttt{ggplot2} is a plotting package that makes it simple to create complex plots from data in a data frame. It provides a more programmatic interface for specifying what variables to plot, how they are displayed, and general visual properties. Therefore, we only need minimal changes if the underlying data change
or if we decide to change from a bar plot to a scatterplot. This helps in creating publication quality plots with minimal amounts of adjustments and tweaking.

\texttt{ggplot} likes data in the 'long' format: i.e., a column for every variable, and a row for every observation. Well structured data will save you lots of time when making figures with \texttt{ggplot}.

\texttt{ggplot} graphics are built step by step by adding new elements. Adding layers in this fashion allows for extensive flexibility and customization of plots.

To build a \texttt{ggplot}, we need to:

\begin{itemize}
\item pipe our data to the \texttt{ggplot()} function
\item define \emph{aesthetics} (\texttt{aes}) by selecting the variables to be plotted and the variables to define the presentation.
\item add \texttt{geoms}, which are graphical representations in the plot (points, lines,bars).
\end{itemize}

We will now construct some examples of what is possible using \texttt{ggplot}:

\begin{verbatim}
data %>%
  ggplot(aes(x=condition, y=RT)) +
  geom_boxplot()
\end{verbatim}

This gives us a vertically oriented boxplot:

\includegraphics[width=.9\linewidth]{figures/week5/boxplot.png}

We can switch the orientation to horizontal by adding the \texttt{coord\_flip()} function.

\begin{verbatim}
data %>%
  ggplot(aes(x=condition, y=RT)) +
  geom_boxplot() +
  coord_flip()
\end{verbatim}

\includegraphics[width=.9\linewidth]{figures/week5/boxplotHorizontal.png}

Boxplots are fine, but they hide the \emph{shape} of the distribution.  Let's try plotting some histograms.

\begin{verbatim}
data %>%
  ggplot(aes(x=RT, group=condition)) +
  geom_histogram(aes(fill=condition))
\end{verbatim}

\includegraphics[width=.9\linewidth]{figures/week5/histogram.png}

In this plot, the histograms are overlaid on the same plot.  We can split them up using \emph{faceting}:

\begin{verbatim}
data %>%
  ggplot(aes(x=RT)) +
  geom_histogram() +
  facet_grid(condition~.)
\end{verbatim}

\includegraphics[width=.9\linewidth]{figures/week5/faceted.png}

Note: density plots can be made in the same way..just use \texttt{geom\_density} instead of \texttt{geom\_histogram}

\subsection*{plotting summaries}
\label{sec-4-1}

Usually, we are interested in the differences between condition means.  We will demonstrate this using two types of plots: a bar plot, and a line plot.

First, lets look at a bar plot that demonstrates the difference in condition means between incongruent and congruent trials.  Notice how we are using the "split-apply-combine" paradigm along with \texttt{ggplot} here:

\begin{verbatim}
data %>%
  group_by(condition) %>%
  summarize(meanRT=mean(RT)) %>%
  ggplot(aes(x=condition,y=meanRT)) +
  geom_bar(stat="identity", width=0.5)
\end{verbatim}

\includegraphics[width=.9\linewidth]{figures/week5/barplot1.png}

Similarly, a small change can produce a line plot instead:

\begin{verbatim}
data %>%
  group_by(condition) %>%
  summarize(meanRT=mean(RT)) %>%
  ggplot(aes(x=condition, y=meanRT, group=1)) +
  geom_line() +
  geom_point()
\end{verbatim}

\includegraphics[width=.9\linewidth]{figures/week5/lineplot1.png}

Notice that this is a terrible plot, though, because the y-axis is truncated.  Adding a line with \texttt{ylim(0,1500)} will make the plot better (try it!)

What if we were interested in the differences in mean RT by distance?  We can easily edit our code above to get bar plots and line plots for distance instead of condition:

Bar plot:

\begin{verbatim}
data %>%
  group_by(distance) %>%
  summarize(meanRT=mean(RT)) %>%
  ggplot(aes(x=distance, y=meanRT)) +
  geom_bar(stat="identity", width=0.5)
\end{verbatim}

\includegraphics[width=.9\linewidth]{figures/week5/barplot2.png}

Line plot:

\begin{verbatim}
data %>%
  group_by(distance) %>%
  summarize(meanRT=mean(RT)) %>%
  ggplot(aes(x=distance, y=meanRT, group=1)) +
  geom_line() +
  geom_point() +
  ylim(0,1500)
\end{verbatim}

\includegraphics[width=.9\linewidth]{figures/week5/lineplot2.png}

Both plots indicate that numerical distance doesn't seem to have much effect on RTs.  

However, a more interesting plot might reveal something different!  Lets see what happens when we plot BOTH condition and distance on the same plot:

Bar plot:

\begin{verbatim}
data %>%
  group_by(condition, distance) %>%
  summarize(meanRT=mean(RT)) %>%
  ggplot(aes(x=distance, y=meanRT, fill=condition)) +
  geom_bar(stat="identity", position=position_dodge(), color="black")
\end{verbatim}

\includegraphics[width=.9\linewidth]{figures/week5/barplot3.png}

Line plot:

\begin{verbatim}
data %>%
  group_by(condition, distance) %>%
  summarize(meanRT=mean(RT)) %>%
  ggplot(aes(x=distance, y=meanRT, linetype=condition)) +
  geom_line() +
  geom_point() +
  ylim(0,1500)
\end{verbatim}

\includegraphics[width=.9\linewidth]{figures/week5/lineplot3.png}

It appears that the effect of condition may be increasing as numerical distance increases.  If so, this is called an \emph{interaction} effect.

\section*{Performing statistical tests}
\label{sec-5}

In this section, we will back up our plots by performing statistical tests.  The tests will perform are the paired samples t-test and a couple of ANOVAs (analysis of variance).

\subsection*{t-test: comparing mean RTs by condition}
\label{sec-5-1}

For our first case study, lets look at the effect of condition and see if we find a significant size-congruity effect.  That is, are response times on incongruent trials significantly longer than congruent trials?

When participants complete multiple experimental trials, the first step is to collapse all trials into single summary stats (usually the mean).  We can use the "split-apply-combine" paradigm to see this.

First, let's see how many participants we had:

\begin{verbatim}
length(unique(data$subject))
\end{verbatim}

Now, split-apply-combine will give us a mean RT for each condition and subject.  That is, we should get 41x2=82 different mean RTs.  We'll add the \texttt{print(n=82)} to the end in order to see all 82 rows.

\begin{verbatim}
data %>%
  group_by(subject, condition) %>%
  summarize(meanRT=mean(RT)) %>%
  print(n=82)
\end{verbatim}

Since "condition" is a within-subjects manipulation (each participant completed both experimental conditions), we'll need to use a paired-samples t-test.  The \texttt{t.test} function in R requires two vectors as input.  We'll use our \texttt{tidyverse} verbs to construct both of these vectors; one for congruent trials, and one for incongruent trials:

\begin{verbatim}
congruent <- data %>%
  group_by(subject, condition) %>%
  summarize(meanRT=mean(RT)) %>%
  filter(condition=="congruent") %>%
  select(subject,meanRT)

incongruent <- data %>%
  group_by(subject, condition) %>%
  summarize(meanRT=mean(RT)) %>%
  filter(condition=="incongruent") %>%
  select(subject,meanRT)
\end{verbatim}

Now, to perform the t-test, we simply type the following:

\begin{verbatim}
t.test(incongruent$meanRT, congruent$meanRT, paired=TRUE)
\end{verbatim}

From the output, we can see there is indeed a significant increase in RTs for incongruent trials, $t(40)=7.48$, $p<0.001$.  Also, we get a 95\% confidence interval for the increase as (36.9, 64.3).

\subsection*{one-way ANOVA: comparing mean RTs by numerical distance}
\label{sec-5-2}

Recall when we plotted mean RT against distance, there did not appear to be much of a difference.  Lets test this with a one-way ANOVA.

The ANOVA procedures are a bit different than the t-test.  First, we collapse all the trials to a single mean RT for each subject x distance combination.  Note: we should expect 41x4=164 different mean RTs.

\begin{verbatim}
dataByDistance <- data %>%
  group_by(subject, distance) %>%
  summarize(meanRT=mean(RT)) %>%
  mutate(distance=as.factor(distance))
\end{verbatim}

Note: the last \texttt{mutate} command is necessary to make the ANOVA work correctly.  All independent variables in an ANOVA model \textbf{must} be "factors".  This command forces R to interpret the numbers 1,2,3,4 as levels of a single categorical factor instead of values of some continuous variable.

Next, we define the ANOVA model:

\begin{verbatim}
distance.aov = aov(meanRT~distance + 
                   Error(as.factor(subject)/distance), 
                   data=dataByDistance
                   )
\end{verbatim}

Note again the \texttt{as.factor()} command; this time, it is used to force the \texttt{subject} variable to be read as categorical.  

Finally, we can see the usual ANOVA table by using the \texttt{summary()} function:

\begin{verbatim}
summary(distance.aov)
\end{verbatim}

The output confirms our suspicion.  There was no effect of distance on mean RTs, $F(3,120)=0.45$, $p=0.72$.

\subsection*{Factorial ANOVA: mean RT by condition and distance}
\label{sec-5-3}

In one of our figures above, we saw that the difference in RTs between congruent and incongruent trials seemed to \emph{increase} as numerical distance increased.  If so, this would be an \emph{interaction} between condition and distance.  To test this, we use a factorial anova.

\begin{verbatim}
dataFactorial <- data %>%
  group_by(subject, condition, distance) %>%
  summarize(meanRT = mean(RT)) %>%
  mutate(distance=as.factor(distance))

factorial.aov = aov(meanRT ~ condition*distance + 
                    Error(as.factor(subject)/(condition*distance)), 
                    data=dataFactorial
                    )
summary(factorial.aov)
\end{verbatim}

As we can see, the overall picture we've suspected is confirmed:

\begin{itemize}
\item there was a significant main effect of condition.  Mean RTs on incongruent trials were significantly longer than congruent trials, $F(1,40)=55.91$, $p<0.001$.
\item there was no effect of distance, $F(3,120)=0.46$, $p=0.71$.
\item however, there was a significant interaction between condition and distance, $F(3,120)=7.86$, $p<0.001$.
\end{itemize}
% Emacs 25.2.1 (Org mode 8.2.10)
\end{document}