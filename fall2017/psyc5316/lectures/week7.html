<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>Week 7 lecture notes - PSYC 5316</title>
<!-- 2017-10-09 Mon 18:10 -->
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="generator" content="Org-mode" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center; }
  .todo   { font-family: monospace; color: red; }
  .done   { color: green; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.right  { text-align: center;  }
  th.left   { text-align: center;   }
  th.center { text-align: center; }
  td.right  { text-align: right;  }
  td.left   { text-align: left;   }
  td.center { text-align: center; }
  dt { font-weight: bold; }
  .footpara:nth-child(2) { display: inline; }
  .footpara { display: block; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/javascript" src="http://orgmode.org/mathjax/MathJax.js"></script>
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
    MathJax.Hub.Config({
        // Only one of the two following lines, depending on user settings
        // First allows browser-native MathML display, second forces HTML/CSS
        //  config: ["MMLorHTML.js"], jax: ["input/TeX"],
            jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        showProcessingMessages: true,
        displayAlign: "center",
        displayIndent: "2em",

        "HTML-CSS": {
             scale: 100,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<h1 class="title">Week 7 lecture notes - PSYC 5316</h1>

<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1">Linear regression modeling</h2>
<div class="outline-text-2" id="text-1">
<p>
This week, we will talk about a classic technique in behavioral and social sciences: linear regression.  After discussing what it is and how to do it (at its most basic form), we will step back and develop TWO techniques for linear regression: one based on <i>minimizing squared errors</i>, and another based on <i>maximum likelihood estimation</i>.
</p>
</div>
</div>

<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2">Linear equations</h2>
<div class="outline-text-2" id="text-2">
<p>
Recall from algebra that any line can be written as \(y=mx+b\), where \(m\) is the slope and \(b\) is the y-intercept.
</p>


<div class="figure">
<p><img src="figures/week7/slopeIntercept.png" alt="slopeIntercept.png" />
</p>
</div>

<p>
In applied contexts, we usually write this equation as \(y=a+bx\).  
</p>
</div>
</div>

<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3">What is a "linear model"?</h2>
<div class="outline-text-2" id="text-3">
<p>
To fit a "linear model" to data, we are making an assumption about the dependency between two variables.  Lets generate some data to see what I'm talking about:
</p>

<pre class="example">
N = 100
x = runif(N)
y = 3 + 5*x + rnorm(N)
plot(x, y, ylim=c(0,10))
</pre>


<div class="figure">
<p><img src="figures/week7/randomLinear.png" alt="randomLinear.png" />
</p>
</div>

<p>
It looks like \(y\) varies linearly with \(x\)..that is, as \(x\) increases, so does \(y\), and the overall pattern looks like a straight line (with noise, of course).
</p>

<p>
So, we define the following model:
</p>

<p>
\[
y_i = a + bx_i + \varepsilon_i
\]
</p>

<p>
Note the index \(i\) represents the different measurements in our sample \(x_i\) and \(y_i\) for \(i=1,\dots, n\), and \(\varepsilon_i\) represents the <b>measurement error</b> on the \(i^{\text{th}}\) measurement.  These are sometimes called <b>residuals</b>.
</p>

<p>
Once we've defined the model, we need to find the <b>parameters</b> of the model: in other words, what are the values for intercept \(a\) and slope \(b\) that <i>best fit</i> the data.  
</p>

<p>
Thus, "fitting a linear model" equates to a "parameter search", where we find the "best fitting" parameters for a given set of data.  To see this in action, we will play around with a simple demo here <a href="https://www.geogebra.org/m/dlsxY1uX">https://www.geogebra.org/m/dlsxY1uX</a>
</p>

<p>
This demo starts with a set of 5 points and a line.  You can change the slope and intercept.  You can also see the residuals (and the squared residuals).  When playing around with this, you should think about what "best fit" would mean.  How do you find the "best fit"?  That is the subject for today's lecture.
</p>
</div>
</div>

<div id="outline-container-sec-4" class="outline-2">
<h2 id="sec-4">Method 1: the "easy" way</h2>
<div class="outline-text-2" id="text-4">
<p>
This is an old problem, and as such, there are well-established methods for solving it.  We will begin by using the base function <code>lm</code> in R to find the parameters that fit the data.
</p>

<pre class="example">
model1 = lm(y~x)
summary(model1)
</pre>


<div class="figure">
<p><img src="figures/week7/lmOutput.png" alt="lmOutput.png" />
</p>
</div>

<p>
The output is quite busy, but there are ways to extract vital information from the model.  First, let's plot the "best fitting line":
</p>

<pre class="example">
intercept = model1$coefficients[1]
slope = model1$coefficients[2]
abline(a=intercept, b=slope)
</pre>

<p>
The first two lines "extract" the intercept (\(a\)) and slope (\(b\)) from the <code>model1</code> object in R.  The third line (<code>abline</code>) plots the line on top of our data!
</p>

<p>
Beyond "fitting" the data quite well, we can use this model for another very important function: <b>prediction</b>.  For example, suppose we wanted to predict the value of \(y\) for a given \(x=0.5\).  Then we can just use the linear equation from our model to predict our value for \(y\):
</p>

\begin{align*}
y & = a + bx\\
  & = 2.87 + 5.10x\\
  & = 2.87 + 5.10(0.5)\\
  & = 5.42\\
\end{align*}

<p>
Lets look at the errors/residuals. 
</p>

<pre class="example">
plot(density(model1$residuals))
</pre>


<div class="figure">
<p><img src="figures/week7/residuals.png" alt="residuals.png" />
</p>
</div>

<p>
Notice that the residuals seem to be normally distributed, centered at 0.  This is good, as it means that, on average, the error in our model is 0, and over-estimates are balanced with under-estimates.  This is generally regarded as a <i>good fit</i>.
</p>
</div>

<div id="outline-container-sec-4-1" class="outline-3">
<h3 id="sec-4-1">Example with real data</h3>
<div class="outline-text-3" id="text-4-1">
<p>
The following data represents some properties of various professions.  We will be interested in the relationship between education and income.
</p>

<pre class="example">
data = read.csv("https://git.io/vd2w3")
plot(data$education, data$income)
</pre>


<div class="figure">
<p><img src="figures/week7/incomePlot.png" alt="incomePlot.png" />
</p>
</div>

<p>
Lets fit a linear model:
</p>

<pre class="example">
model2 = lm(data$income ~ data$education)
summary(model2)
intercept = model2$coefficients[1]
slope = model2$coefficients[2]
abline(a=intercept, b=slope)
</pre>


<div class="figure">
<p><img src="figures/week7/incomeFitted.png" alt="incomeFitted.png" />
</p>
</div>

<p>
We can see that the line looks like a good fit, but lets look at the residuals.
</p>

<pre class="example">
plot(density(model2$residuals))
</pre>


<div class="figure">
<p><img src="figures/week7/incomeResiduals.png" alt="incomeResiduals.png" />
</p>
</div>

<p>
The residuals peak at 0 (so most of our measurement error is 0), but we seem to have more underestimates (positive residuals) than overestimates (negative residuals).  Thus, this is not a <i>great</i> fit, but OK.
</p>
</div>
</div>
</div>

<div id="outline-container-sec-5" class="outline-2">
<h2 id="sec-5">Method 2 &#x2013; minimizing squared error</h2>
<div class="outline-text-2" id="text-5">
<p>
We will now talk about <b>how</b> to fit a linear regression model.  The first method we will discuss is the classical "OLS" method (ordinary least squares).  The basic idea is to compute errors between actual and predicted values, square them to get rid of negatives, and then find the parameters \(a\) and \(b\) which minimize this "squared error".
</p>

<p>
Mathematically, we want to find parameters \(a\) and \(b\) that minimize:
</p>

<p>
\[
\sum \varepsilon_i^2 = \sum (y_i - (a+bx_i))^2
\]
</p>

<p>
We can do this in R using the <code>optim</code> command.
</p>

<p>
First, we define a function that calculates the sum of squared errors:
</p>

<pre class="example">
SS = function(data,par){
  with(data, sum((y-(par[1]+par[2]*x))^2))
}
</pre>

<p>
As we did with maximum likelihood estimation in Week 2, this function takes two arguments: <code>data</code> and <code>par</code>.  In this context, <code>data</code> will be a data frame with two columns (<code>x</code> and <code>y</code>), and <code>par</code> will be a vector containing our parameters \(a\) (<code>par[1]</code>) and \(b\) (<code>par[2]</code>).
</p>

<p>
To find \(a\) and \(b\) that <b>minimize</b> this sum-of-squares function, we will use <code>optim</code> with a reasonable guess for initial values:
</p>

<pre class="example">
dat=data.frame(x,y)
inits=c(1,1)
optim(inits, SS, data=dat)
</pre>


<div class="figure">
<p><img src="figures/week7/SS.png" alt="SS.png" />
</p>
</div>

<p>
As you might expect, our fitted parameters \(a\) and \(b\) are the same as we got when we used the <code>lm</code> function earlier.
</p>

<p>
Lets try this with our other example (the education versus salary data):
</p>

<pre class="example">
dat=data.frame(data$education, data$income)
names(dat) = c("x", "y")
inits=c(1,1)
optim(inits, SS, data=dat)
</pre>

<p>
As you will see, the parameter estimates for \(a\) and \(b\) are almost exactly the same as we obtained with the <code>lm</code> command.
</p>
</div>
</div>

<div id="outline-container-sec-6" class="outline-2">
<h2 id="sec-6">Method 3 &#x2013; maximum likelihood estimation</h2>
<div class="outline-text-2" id="text-6">
<p>
Instead of minimizing the sum of squared errors, we can use maximum likelihood estimation.  This requires a bit more sophistication in our definition of a <b>linear model</b>.  So, lets start there.
</p>

<p>
Recall that for MLE, one needs a likelihood function.  That is, we need some sort of distributional assumption to proceed (e.g., is something normally distributed)?  Recall that in a linear model, we have
</p>

<p>
\[
y_i = a+bx_i + \varepsilon_i
\]
</p>

<p>
or rewritten
</p>

<p>
\[
\varepsilon_i = y_i - (a+bx_i)
\]
</p>

<p>
We want the residuals \(\varepsilon_i\) to be centered at 0.  Thus, one way to do this is to place a <b>normal</b> model on the residuals.  That is, we assume
</p>

<p>
\[
\varepsilon_i \sim \text{Normal}(0, \sigma^2)
\]
</p>

<p>
So, this is a problem that MLE can solve!  We simply need to find parameters \(a\) and \(b\) so that \(y_i-(a+bx_i)\) is normally distributed with mean 0 and variance \(\sigma^2\).
</p>

<p>
As in week 2, we begin by defining a negative log-likelihood (NLL) function:
</p>

<pre class="example">
reg.nll = function(data, par){
  residual = with(data, y-(par[1]+par[2]*x))
  return(sum(-log(dnorm(residual, mean=0, sd=par[3]))))
}
</pre>

<p>
Then, we minimize this NLL via the <code>optim</code> function.
</p>

<pre class="example">
dat=data.frame(x,y)
inits=c(3,5,2)
optim(inits, reg.nll, data=dat)
</pre>


<div class="figure">
<p><img src="figures/week7/mle.png" alt="mle.png" />
</p>
</div>

<p>
Notice that we get the same estimates for \(a\) and \(b\) as before.  This time, we also get an estimate for \(\sigma\), the standard deviation of the residual distribution.
</p>
</div>

<div id="outline-container-sec-6-1" class="outline-3">
<h3 id="sec-6-1">Confidence intervals on parameter estimates</h3>
<div class="outline-text-3" id="text-6-1">
<p>
One of the advantages to the MLE approach is that there is a straightforward compute confidence intervals for our estimates.  We need an estimate for the <b>standard error</b> of our MLE.  This is given to us (quite indirectly) by the <i>Hessian</i> matrix, which is an output option for <code>optim</code>.  
</p>

<p>
Intuitively, the standard error of the estimate is given by:
</p>

<p>
\[
SE = \sqrt{\frac{1}{\text{Hessian}}}
\]
</p>

<p>
This is not <i>exactly</i> true, since in most cases the Hessian will be a <i>matrix</i>, and square roots and reciprocals are not exactly obvious in matrix arithmetic.  However, if you are willing to assume some linear algebra "happens", the following R commands will give you a nice estimate for the standard error:
</p>

<pre class="example">
fit = optim(inits, reg.nll, data=dat, hessian = TRUE)
hessian = fit$hessian
inverse = solve(hessian)
se = sqrt(diag(inverse))
</pre>

<p>
Now, if we call <code>se</code> in the console, we'll see a vector with three numbers.  These are the standard error estimates for \(a\), \(b\), and \(\sigma\), respectively.
</p>

<pre class="example">
&gt; se
[1] 0.21479319 0.38639248 0.07918032
</pre>

<p>
Thus, we can construct 95% confidence intervals for the intercept \(a\):
</p>

<p>
\[
\Bigl(2.87 - 1.96(0.21), 2.87+1.96(0.21)\Bigr) = (2.45, 3.29)
\]
</p>

<p>
as well as for the slope \(b\):
</p>

<p>
\[
\Bigl(5.10 - 1.96(0.39), 5.10+1.96(0.39)\Bigr) = (4.34, 5.86)
\]
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: October 9, 2017</p>
<p class="date">Created: 2017-10-09 Mon 18:10</p>
<p class="creator"><a href="http://www.gnu.org/software/emacs/">Emacs</a> 25.2.1 (<a href="http://orgmode.org">Org</a> mode 8.2.10)</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
