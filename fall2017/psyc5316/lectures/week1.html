<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>Week 1 lecture notes - PSYC 5316</title>
<!-- 2017-08-28 Mon 12:54 -->
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="generator" content="Org-mode" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center; }
  .todo   { font-family: monospace; color: red; }
  .done   { color: green; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.right  { text-align: center;  }
  th.left   { text-align: center;   }
  th.center { text-align: center; }
  td.right  { text-align: right;  }
  td.left   { text-align: left;   }
  td.center { text-align: center; }
  dt { font-weight: bold; }
  .footpara:nth-child(2) { display: inline; }
  .footpara { display: block; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/javascript" src="http://orgmode.org/mathjax/MathJax.js"></script>
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
    MathJax.Hub.Config({
        // Only one of the two following lines, depending on user settings
        // First allows browser-native MathML display, second forces HTML/CSS
        //  config: ["MMLorHTML.js"], jax: ["input/TeX"],
            jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        showProcessingMessages: true,
        displayAlign: "center",
        displayIndent: "2em",

        "HTML-CSS": {
             scale: 100,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<h1 class="title">Week 1 lecture notes - PSYC 5316</h1>

<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1">Basic definitions</h2>
<div class="outline-text-2" id="text-1">
</div><div id="outline-container-sec-1-1" class="outline-3">
<h3 id="sec-1-1">probability functions</h3>
<div class="outline-text-3" id="text-1-1">
<p>
Our goal this semester is to develop a toolkit for advanced statistical inference.
</p>

<p>
All of statistical inference deals with statements of <i>probability</i>.  But what exactly is probability?
</p>

<p>
Apparently people are not too sure!  The following data comes from an online replication of the classic Sherman Kent study on perceptions of uncertainty.  People were asked to assign a percentage of probability to different phrases.
</p>

<p>
Results:
</p>


<div class="figure">
<p><img src="figures/probPlot.png" alt="probPlot.png" />
</p>
</div>

<p>
As you can see, there is a lot of variability in our perceptions of uncertainty.  Thus, it is very important to know <i>exactly</i> what we are talking about when dealing with probability.  We will be very specific about these notions this semester.
</p>

<p>
So, to start &#x2013; What exactly is probability?
</p>

<p>
Definition: a <b>probability</b> is a function \(p(x)\) that assigns "outcomes" \(x\) to real numbers \(p(x)\) so that the following properties hold:
</p>

<ol class="org-ol">
<li>\(p(x)\geq 0\) for any \(x\)
</li>
<li>For any two mutually exclusive outcomes \(x\) and \(y\), we have \(p(x\text{ or }y)=p(x)+p(y)\)
</li>
<li>\(\sum p(x) = 1\), where the sum is taken over all possible outcomes \(x\)
</li>
</ol>

<p>
Example: verify that the following mapping defines a probability:
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="left" />

<col  class="right" />

<col  class="right" />

<col  class="right" />

<col  class="right" />
</colgroup>
<tbody>
<tr>
<td class="left">\(x\)</td>
<td class="right">0</td>
<td class="right">1</td>
<td class="right">2</td>
<td class="right">3</td>
</tr>

<tr>
<td class="left">\(p(x)\)</td>
<td class="right">0.125</td>
<td class="right">0.375</td>
<td class="right">0.375</td>
<td class="right">0.125</td>
</tr>
</tbody>
</table>

<p>
Note that we can also construct a bar plot of this probability:
</p>

<pre class="example">
x &lt;- c(0, 1, 2, 3)
p &lt;- c(0.125, 0.375, 0.375, 0.125)
barplot(p, names.arg=x, xlab="x", ylab="p(x)")
</pre>


<div class="figure">
<p><img src="figures/week1/barplot.png" alt="barplot.png" />
</p>
</div>


<p>
This bar plot forms an important visualization tool for us called a <b>probability distribution</b>.  We'll talk more about this a bit later.
</p>
</div>
</div>

<div id="outline-container-sec-1-2" class="outline-3">
<h3 id="sec-1-2">expected value and variance</h3>
<div class="outline-text-3" id="text-1-2">
<p>
Expected value is a generalization of the concept of mean (which you are already quite familiar with).
</p>

<p>
Definition: the <b>expected value</b> of a random variable \(X\) is given by
</p>

<p>
\[
E(X) = \sum_{x\in X}xp(x)
\]
</p>

<p>
Expected value is found by multiplying each outcome by its associated probability, and then adding together all resulting products.
</p>

<p>
Example: consider the example we just did above.  Then
</p>

<p>
\[
E(X) = \sum_{x\in X}xp(x) = 0\cdot 0.125 + 1\cdot 0.375 +2\cdot 0.375 + 3\cdot 0.125 = 1.5
\]
</p>

<p>
Note: you can also get the same thing in R very quickly via the following command:
</p>

<pre class="example">
sum(x*p)
</pre>

<p>
Expected value is a useful concept in many ways:
</p>
<ul class="org-ul">
<li>it allows us to define a notion of "mean" for ANY set of outcomes (even an infinite set)
</li>
<li>it gives us a very quick definition of variance!
</li>
</ul>

<p>
Example: the <b>variance</b> of a random variable \(X\) is defined as \(E[(x-\mu)^2]\).
</p>

<p>
Before we compute this for our example distribution, lets check that it makes sense.  \(E[(x-\mu)^2]\) represents our "expectation" of the squared difference from the mean.  That is exactly how variance was defined in your past statistics courses.
</p>

<p>
Example: consider again our example distribution from earlier.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="left" />

<col  class="right" />

<col  class="right" />

<col  class="right" />

<col  class="right" />
</colgroup>
<tbody>
<tr>
<td class="left">\(x\)</td>
<td class="right">0</td>
<td class="right">1</td>
<td class="right">2</td>
<td class="right">3</td>
</tr>

<tr>
<td class="left">\(p(x)\)</td>
<td class="right">0.125</td>
<td class="right">0.375</td>
<td class="right">0.375</td>
<td class="right">0.125</td>
</tr>

<tr>
<td class="left">\(x-\mu\)</td>
<td class="right">-1.5</td>
<td class="right">0.5</td>
<td class="right">0.5</td>
<td class="right">1.5</td>
</tr>

<tr>
<td class="left">\((x-\mu)^2\)</td>
<td class="right">2.25</td>
<td class="right">0.25</td>
<td class="right">0.25</td>
<td class="right">2.25</td>
</tr>

<tr>
<td class="left">\((x-\mu)^2\cdot p(x)\)</td>
<td class="right">0.28125</td>
<td class="right">0.09375</td>
<td class="right">0.09375</td>
<td class="right">0.28125</td>
</tr>
</tbody>
</table>

<p>
So the variance is \(E[(x-\mu)^2]\) = 0.28125+0.09375+0.98375+0.28125 = 0.75
</p>

<p>
As before, this is pretty easy to do in R:
</p>

<pre class="example">
mu = sum(x*p)
sum((x-mu)^2*p)
</pre>
</div>
</div>
</div>

<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2">Special Distributions</h2>
<div class="outline-text-2" id="text-2">
<p>
Many common probability distributions are given by explicit formulas for their probability functions.  Two that we'll talk about this week are the <b>binomial</b> and <b>normal</b> distributions.
</p>
</div>
<div id="outline-container-sec-2-1" class="outline-3">
<h3 id="sec-2-1">Binomial distribution</h3>
<div class="outline-text-3" id="text-2-1">
<p>
The binomial distribution goes back to Bernoulli in 1713.  It arises in situations where each experimental trial has only two outcomes, which we'll call success and failure (such trials are called Bernoulli trials).  
</p>

<p>
Before presenting the general form of the binomial, lets try a simple example.
</p>

<p>
Suppose we flipped a fair coin three times.  The probability of landing heads (which we'll call a "success") is \(p=0.5\).  Consequently, the probability of landing tails (which we'll call "failure") is \(1-p=0.5\).
</p>

<p>
Let \(X\) denote the random variable that counts the number of successes in these three trials.
</p>

<p>
Some questions:
</p>
<ol class="org-ol">
<li>what are the possible values (outcomes) for \(X\)?
</li>
<li>what is the probability of each outcome?
</li>
</ol>

<p>
Answers:
</p>
<ol class="org-ol">
<li>we can get 0, 1, 2, or 3 heads. 
</li>
<li>let's make a table:
</li>
</ol>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="right" />

<col  class="left" />

<col  class="left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="right">outcome</th>
<th scope="col" class="left">arrangements</th>
<th scope="col" class="left">probability</th>
</tr>
</thead>
<tbody>
<tr>
<td class="right">0</td>
<td class="left">TTT</td>
<td class="left">1/8 = 0.125</td>
</tr>

<tr>
<td class="right">1</td>
<td class="left">HTT, THT, TTH</td>
<td class="left">3/8 = 0.375</td>
</tr>

<tr>
<td class="right">2</td>
<td class="left">THH, HTH, HHT</td>
<td class="left">3/8 = 0.375</td>
</tr>

<tr>
<td class="right">3</td>
<td class="left">HHH</td>
<td class="left">1/8 = 0.125</td>
</tr>
</tbody>
</table>

<p>
Hence, we get the same probability distribution that we were working with earlier:
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="left" />

<col  class="right" />

<col  class="right" />

<col  class="right" />

<col  class="right" />
</colgroup>
<tbody>
<tr>
<td class="left">\(x\)</td>
<td class="right">0</td>
<td class="right">1</td>
<td class="right">2</td>
<td class="right">3</td>
</tr>

<tr>
<td class="left">\(p(x)\)</td>
<td class="right">0.125</td>
<td class="right">0.375</td>
<td class="right">0.375</td>
<td class="right">0.125</td>
</tr>
</tbody>
</table>

<p>
It is not too difficult to come up with a mathematical process that generates these probabilities for the general case.
</p>

<p>
Suppose we have \(N\) trials, and the probability of success on any one trials is \(\theta\).  
</p>

<p>
Then we know the following:
</p>
<ol class="org-ol">
<li>the probability of each success is \(\theta\)
</li>
<li>the probability of each failure is \(1-\theta\)
</li>
<li>we have \(x\) successes
</li>
<li>we have \(N-x\) failures
</li>
<li>there are \({N\choose x}=\frac{N!}{x!(N-x)!}\) ways to arrange these \(x\) successes from the \(N\) trials
</li>
</ol>

<p>
Thus, the general formula for the binomial probability function is:
</p>

<p>
\[
p(x) = {N\choose x} \theta^x(1-\theta)^{N-x}
\]
</p>
</div>

<div id="outline-container-sec-2-1-1" class="outline-4">
<h4 id="sec-2-1-1">Example 1</h4>
<div class="outline-text-4" id="text-2-1-1">
<p>
A coin is tossed 10 times.  What is the probability of getting exactly 7 heads?
</p>

<p>
This problem corresponds to a binomial experiment, where there are 10 independent Bernoulli trials, and \(\theta=0.5\).  Then we can calculate the probability via the binomial distribution:
</p>

<p>
\[
p(x=7) = {10\choose 7} (0.5)^7(0.5)^3
\]
</p>

<p>
Using R, we can calculate this as:
</p>

<pre class="example">
choose(10,7)*0.5^7*0.5^3
</pre>

<p>
As we can see from the answer (0.117), this outcome is fairly unlikely
</p>
</div>
</div>

<div id="outline-container-sec-2-1-2" class="outline-4">
<h4 id="sec-2-1-2">Example 2</h4>
<div class="outline-text-4" id="text-2-1-2">
<p>
Suppose you run a series of 5 independent replications of an experiment, each with fairly high power (say 80%).  Suppose there is a true effect; that is, the null hypothesis is false.  What is the probability of getting a significant result (i.e., rejecting the null) in all 5 experiments?
</p>

<p>
This situation can also be thought of using a binomial distribution.  Each experiment is an independent trial with two outcomes: reject null, or fail to reject null.  The probability of rejecting the null is 0.8.  Then by the binomial distribution, we have
</p>

<p>
\[
p(x=5) = {5\choose 5} (0.8)^5(0.2)^0 = 0.328
\]
</p>

<p>
(this might be surprising!)
</p>
</div>
</div>

<div id="outline-container-sec-2-1-3" class="outline-4">
<h4 id="sec-2-1-3">using R</h4>
<div class="outline-text-4" id="text-2-1-3">
<p>
While we can do binomial calcuations from scratch, R has some nice built-in functions to handle these calculations.  Any of the above calculations can be done using the <code>dbinom</code> function.  As an illustration, the two previous examples could be computed using the following commands:
</p>

<pre class="example">
dbinom(x=7, size=10, prob=0.5)
dbinom(x=5, size=5, prob=0.8)
</pre>
</div>
</div>

<div id="outline-container-sec-2-1-4" class="outline-4">
<h4 id="sec-2-1-4">Example 3</h4>
<div class="outline-text-4" id="text-2-1-4">
<p>
We can use the binomial distribution for an easy hypothesis test. 
</p>

<p>
A common experiment in psychophysics involves judgments of relative magnitude.  Suppose participants were asked to pick the larger of two visually presented line segments.  On each trial, the pair was presented for a very brief time (150 ms).  
</p>

<p>
Suppose a participant was correct on 9 out of 10 of these trials.  Can we conclude that the participant was operating at "better than chance" level (e.g., that the participant was not guessing)?
</p>

<p>
Here, the null hypothesis would be that the participant was guessing, so each trial would have a 50% chance of success.  That is, \(H_0: \theta=0.5\).  The alternative hypothesis would be that the actual probability of success is greater than 50%.  That is, \(H_1: \theta>0.5\).
</p>

<p>
Suppose the null (that is, \(\theta=0.5\)).  We will compute the probability of observing 8 or more successes:
</p>

<p>
\[
p(x\geq 9) = p(x=9) + p(x=10)
\]
</p>

<p>
For simplicity, we will compute these probabilities in R:
</p>

<pre class="example">
dbinom(x=9, size=10, prob=0.5) + dbinom(x=10, size=10, prob=0.5)
</pre>

<p>
We get an answer of 0.01.  That is, if participants were guessing, then the probability of observing 9 or more correct responses is only 0.01.  Therefore, we reject the null hypothesis (\(\theta=0.5\)) in favor of the alternative hypothesis (\(\theta>0.5\)).
</p>
</div>
</div>

<div id="outline-container-sec-2-1-5" class="outline-4">
<h4 id="sec-2-1-5">Digging deeper &#x2013; Likelihoods</h4>
<div class="outline-text-4" id="text-2-1-5">
<p>
If we have time, we can talk about an extension of the binomial distribution.  Suppose we flip a coin 10 times, and we see 8 heads.  Do we believe this coin is fair?
</p>

<p>
Notice that this is a different problem than before.  Here, we are given the number of successes, but NOT the probability of success on any one trial.  This is called a <b>likelihood</b> function.  Let's plot it:
</p>

<pre class="example">
x = seq(from=0, to=1, by=0.01)
y = dbinom(x=8, size=10, prob=x)
plot(x, y, type="l", xlab="p", ylab="likelihood")
</pre>


<div class="figure">
<p><img src="figures/week1/likelihood.png" alt="likelihood.png" />
</p>
</div>

<p>
This curve tells us which values of \(p\) are most likely to give us the data we observed; namely, 8 successes.  Which value of \(p\) do you think is <i>most likely</i>?
</p>

<p>
You can test your prediction by adding the command <code>abline(v=est)</code>, where <code>est</code> is the number for \(p\) you estimated.
</p>


<div class="figure">
<p><img src="figures/week1/mle.png" alt="mle.png" />
</p>
</div>

<p>
Note: this estimator (0.8 in this case) is called the <b>maximum likelihood estimate</b>.  It is literally the estimate for \(p\) which maximizes the likelihood function.  That means it is the estimate for \(p\) that is most likely, given the observed data.  This is a fundamental tool in statistical modeling.
</p>
</div>
</div>
</div>

<div id="outline-container-sec-2-2" class="outline-3">
<h3 id="sec-2-2">Normal distribution</h3>
<div class="outline-text-3" id="text-2-2">
<p>
Like the binomial, the normal distribution is also a probability distribution.  Its probability function is given by:
</p>

<p>
\[
f(x)=\frac{1}{\sigma\sqrt{2\pi}}\exp \Bigl( -\frac{(x-\mu)^2}{2\sigma^2}\Bigr)
\]
</p>

<p>
Note: it can be shown that:
</p>
<ul class="org-ul">
<li>\(E[x]=\mu\)
</li>
<li>$E[(x-&mu;)<sup>2]</sup> = \(\sigma^2\)
</li>
</ul>

<p>
To plot a normal distribution in R, we can use the <code>dnorm</code> function.  For example, lets plot a normal distribution with mean 100 and standard deviation 15.
</p>

<pre class="example">
x = seq(from=0, to=200, by=0.1)
y = dnorm(x, mean=100, sd=15)
plot(x, y, type="l")
</pre>

<p>
For fun, we can add another normal curve to this plot that has the same mean, but different variance.  Try adding the following..what do you notice:
</p>

<pre class="example">
lines(x,dnorm(x, mean=100, sd=30), lty=2)
</pre>
</div>

<div id="outline-container-sec-2-2-1" class="outline-4">
<h4 id="sec-2-2-1">computing normal probabilities</h4>
<div class="outline-text-4" id="text-2-2-1">
<p>
The R commands <code>pnorm</code> and <code>qnorm</code> makes computing probabilities with the normal distribution very easy.
</p>

<p>
<b>pnorm</b>: The command <code>pnorm(q, mean=0, sd=1)</code> returns the probability that a normal random variable has value less than or equal to <code>q</code>, given a specific value for <code>mean</code> and <code>sd</code>.  
</p>

<p>
Example: suppose that cholesterol levels in adults are normally distributed with mean \(\mu=230\) and standard deviation \(\sigma=20\).  
</p>

<p>
What is the probability that a randomly selected adult will have a cholesterol level 
</p>

<ul class="org-ul">
<li>less than or equal to 200?
</li>
</ul>

<pre class="example">
pnorm(200, mean=230, sd=20)
</pre>

<ul class="org-ul">
<li>greater than or equal to 240?
</li>
</ul>

<pre class="example">
1-pnorm(240, mean=230, sd=20)
</pre>

<ul class="org-ul">
<li>between 210 and 250
</li>
</ul>

<pre class="example">
pnorm(250, mean=230, sd=20) - pnorm(210, mean=230, sd=20)
</pre>


<p>
<b>qnorm</b>: the command <code>qnorm(p, mean=0, sd=1)</code> returns the "quantile" (or "percentile") associated with probability \(p\) in a normal distribution with a specific <code>mean</code> and <code>sd</code>.
</p>

<p>
Example:  In the cholesterol example above, what cholesterol level would be greater than or equal to 95% of the population?
</p>

<pre class="example">
qnorm(0.95, mean=230, sd=20)
</pre>

<p>
What interval would contain 95% of the cholesterol levels in the population?
</p>

<pre class="example">
qnorm(c(0.025, 0.975), mean=230, sd=20)
</pre>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: August 28, 2017</p>
<p class="date">Created: 2017-08-28 Mon 12:54</p>
<p class="creator"><a href="http://www.gnu.org/software/emacs/">Emacs</a> 25.2.1 (<a href="http://orgmode.org">Org</a> mode 8.2.10)</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
