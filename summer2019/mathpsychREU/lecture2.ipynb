{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lecture2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomfaulkenberry/courses/blob/master/summer2019/mathpsychREU/lecture2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEFXQAo1069O",
        "colab_type": "text"
      },
      "source": [
        "# Lecture 2 - Fitting a \"forgetting curve\"\n",
        "\n",
        "It is well known that once we learn something, we tend forget some things as time passes.  \n",
        "\n",
        "Murdock (1961) presented subjects with a set of memory items (i.e., words or letters) and asked them to recall the items after six different retention intervals: $t=1,3,6,9,12,18$ (in seconds).  He recorded the proportion recalled at each retention interval (based on 100 independent trials for each $t$). These data were (respectively)\n",
        "\n",
        "$$\n",
        "y=0.94, 0.77, 0.40, 0.26, 0.24, 0.16\n",
        "$$\n",
        "\n",
        "**Our goal**: fit a mathematical model that will predict the proportion recalled $y$ as a function of retention interval ($t$)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XB8jNbDz4ZLn",
        "colab_type": "text"
      },
      "source": [
        "## First step - look at the data!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HlS9agjrypv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from numpy import *\n",
        "\n",
        "T = array([1,3,6,9,12,18])\n",
        "Y = array([0.94, 0.77, 0.40, 0.26, 0.24, 0.16])\n",
        "\n",
        "plt.plot(T, Y, 'o')\n",
        "plt.xlabel('Retention interval (sec.)')\n",
        "plt.ylabel('Proportion recalled')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxjTCx3C5IoZ",
        "colab_type": "text"
      },
      "source": [
        "Some things to notice:\n",
        "\n",
        "*   our model should be a *decreasing* function\n",
        "*   it is NOT linear\n",
        "\n",
        "Two candidate models:\n",
        "\n",
        "*   Power function model: $y=ax^b$\n",
        "*   Exponential model: $y=ab^x$\n",
        "\n",
        "Which one should we use?\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEkMpfSR61nU",
        "colab_type": "text"
      },
      "source": [
        "## mathematical properties?\n",
        "\n",
        "Take logs and look at structure of data\n",
        "\n",
        "\n",
        "Power function model:  $\\ln y = \\ln a + b\\ln x$\n",
        "* so power $\\implies$ $\\ln y$ should be *linear* wrt $\\ln x$\n",
        "\n",
        "\n",
        "Exponential model: $\\ln y = \\ln a + x\\ln b$ \n",
        "* so exponential $\\implies$ $\\ln y$ should be *linear* wrt $x$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAbmoYrP79Xg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check power function model\n",
        "\n",
        "plt.plot(log(T), log(Y), 'o')\n",
        "plt.xlabel('$\\ln t$')\n",
        "plt.ylabel('$\\ln y$')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YC26AhgT81GG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check exponential model\n",
        "\n",
        "plt.plot(T, log(Y), 'o')\n",
        "plt.xlabel('$t$')\n",
        "plt.ylabel('$\\ln y$')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00ocZfkS8-zo",
        "colab_type": "text"
      },
      "source": [
        "Both are reasonably linear, but neither is a perfect fit!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcmOreah9CeS",
        "colab_type": "text"
      },
      "source": [
        "## Fit both models with MLE \n",
        "\n",
        "At this point, our best bet is to find parameter sets for both models that provide best fit to observed data $y$.  We will use maximum likelihood estimation.\n",
        "\n",
        "### Step 1 -- compute the likelihood function\n",
        "\n",
        "First, let's cast our data as the number of items recalled correctly on $n=100$ trials.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKpJS7DTHkc_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = 100*Y\n",
        "print(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSH5Pi2_Ho7-",
        "colab_type": "text"
      },
      "source": [
        "Let's assume each of these 100 trials is independent of the others, and consider each trial a *success* if item is correctly recalled.\n",
        "\n",
        "Then the probability of correctly recalling $x$ items is:\n",
        "\n",
        "$$\n",
        "f(x\\mid\\theta) = \\binom{100}{x}\\theta^x(1-\\theta)^{100-x}\n",
        "$$\n",
        "\n",
        "The critical parameter here is $\\theta$ -- the probability of success on any *one* trial.  How do we determine $\\theta$?\n",
        "\n",
        "Let's assume that probability of recall is governed by a **power function**.  That is, assume\n",
        "\n",
        "$$\n",
        "\\theta(t) = at^b\n",
        "$$\n",
        "\n",
        "for constants $a,b$.\n",
        "\n",
        "Then we can write\n",
        "$$\n",
        "f(x\\mid a,b) = \\binom{100}{x}(at^b)^x(1-at^b)^{100-x}\n",
        "$$\n",
        "\n",
        "which we cast as a likelihood\n",
        "\n",
        "$$\n",
        "L(a,b\\mid x) = \\binom{100}{x}(at^b)^x(1-at^b)^{100-x}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BREYCezeJnwK",
        "colab_type": "text"
      },
      "source": [
        "### Step 2 -- compute log likelihood\n",
        "\n",
        "This gives us:\n",
        "\n",
        "$$\n",
        "\\ln L = \\ln \\Biggl[ \\binom{100}{x}\\Biggr] + x\\ln(at^b) + (100-x)\\ln(1-at^b)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dipz_c0iLQc1",
        "colab_type": "text"
      },
      "source": [
        "### Step 3 -- extend to multiple observations\n",
        "\n",
        "Note that the formula above is for a *single* observation $x$.  But we have 5 observations!\n",
        "\n",
        "If we assume each is independent from the others, then we can multiply the likelihoods:\n",
        "\n",
        "$$\n",
        "L(a,b\\mid x=(x_1,\\dots,x_5)) = \\prod_{i=1}^5 L(a,b\\mid x_i)\n",
        "$$\n",
        "\n",
        "Thus we have\n",
        "\n",
        "$$\n",
        "\\ln L = \\ln\\Biggl(\\prod_{i=1}^5 L(a,b\\mid x_i)\\Biggr )\n",
        "$$\n",
        "\n",
        "But since logs turn products into sums, we can write\n",
        "\n",
        "$$ \\ln L = \\sum_{i=1}^5 \\ln L(a,b\\mid x_i) = \\sum_{i=1}^5 \\Biggl(\\ln \\binom{100}{x_i} + x_i\\ln(at^b) + (100-x_i)\\ln(1-at^b)\\Biggr)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04ITB1M9PpX0",
        "colab_type": "text"
      },
      "source": [
        "*Notes:*\n",
        "\n",
        "* we really only care about the terms that have $a$ and $b$, so we'll ignore the binomial term\n",
        "* Python really likes to *minimize*. So, we will minimize the *negative* log likelihood (NLL)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq7uCTOQtMLj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nllP(pars):\n",
        "  a, b = pars\n",
        "  tmp1 = X*log(a*T**b) \n",
        "  tmp2 = (100-X)*log(1-a*T**b)\n",
        "  return(-1*sum(tmp1+tmp2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oWELP1O_Ezp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check some examples\n",
        "\n",
        "a = 0.7\n",
        "b = -0.3\n",
        "pars = array([a,b])\n",
        "\n",
        "nllP(pars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-ffE9Di_Wfh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.optimize import minimize\n",
        "\n",
        "a_init = random.uniform()\n",
        "b_init = -random.uniform()\n",
        "inits = array([a_init, b_init])\n",
        "\n",
        "mle = minimize(nllP, \n",
        "               init,\n",
        "               method=\"nelder-mead\")\n",
        "print(mle)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awB1EgVdDbdz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def power(t,pars):\n",
        "  a, b = pars\n",
        "  return(a*t**b)\n",
        "\n",
        "fitPars = mle.x\n",
        "print(f\"a={fitPars[0]:.3f}, b={fitPars[1]:.3f}\")\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "x = linspace(0.5,18,100)\n",
        "ax.plot(T,Y,'o')\n",
        "ax.plot(x, power(x,fitPars))\n",
        "\n",
        "plt.show()\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WHhnNMkWkjN",
        "colab_type": "text"
      },
      "source": [
        "## Exercises\n",
        "\n",
        "1. Often, the \"power law of forgetting\" is written as $f(t) = at^{-b}$ (e.g., Wixted, 1990), the purpose of which is to reinforce the idea that forgetting = decay.  What does this change do to the likelihood function?  Use Python to compute MLEs for $a$ and $b$ given the observed data $y$ above.\n",
        "\n",
        "2. Demonstrate (either through computation or a mathematical proof) that we can safely ignore the $\\binom{100}{x_i}$ term in the likelihood function.\n",
        "\n",
        "3. Rubin and Baddeley (1989) measured the proportion of participants who correctly recalled details from a past colloquium talk as a function of time in years.  The data below are approximately equal to what they orignally found:\n",
        "\n",
        "|time (years) | proportion recall|\n",
        "|:--:|:--:|\n",
        "| 0.05 | 0.38|\n",
        "|0.25 | 0.26|\n",
        "|0.30|0.22|\n",
        "|0.60|0.20|\n",
        "|0.95|0.11|\n",
        "|1.3|0.07|\n",
        "|1.4|0.16|\n",
        "|1.6|0.10|\n",
        "|1.8|0.08|\n",
        "|2.5|0.05|\n",
        "|2.7|0.01|\n",
        "\n",
        "For simplicity, assume there were 100 participants. Construct a reasonable model of forgetting for this data, and estimate its parameters.\n"
      ]
    }
  ]
}